AdaptiveSparkPlan isFinalPlan=false
+- Sort [ext_price#41157 DESC NULLS LAST, brand_id#41155 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(ext_price#41157 DESC NULLS LAST, brand_id#41155 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=138653]
      +- HashAggregate(keys=[i_brand#1279, i_brand_id#1278, t_hour#39579, t_minute#39580], functions=[sum(ext_price#41143)], output=[brand_id#41155, brand#41156, t_hour#39579, t_minute#39580, ext_price#41157])
         +- Exchange hashpartitioning(i_brand#1279, i_brand_id#1278, t_hour#39579, t_minute#39580, 200), ENSURE_REQUIREMENTS, [plan_id=138650]
            +- HashAggregate(keys=[i_brand#1279, i_brand_id#1278, t_hour#39579, t_minute#39580], functions=[partial_sum(ext_price#41143)], output=[i_brand#1279, i_brand_id#1278, t_hour#39579, t_minute#39580, sum#41238])
               +- Project [i_brand_id#1278, i_brand#1279, ext_price#41143, t_hour#39579, t_minute#39580]
                  +- BroadcastHashJoin [time_sk#41146], [t_time_sk#39576], Inner, BuildRight, false
                     :- Project [i_brand_id#1278, i_brand#1279, ext_price#41143, time_sk#41146]
                     :  +- BroadcastHashJoin [i_item_sk#1271], [sold_item_sk#41145], Inner, BuildLeft, false
                     :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=138641]
                     :     :  +- Project [i_item_sk#1271, i_brand_id#1278, i_brand#1279]
                     :     :     +- Filter ((isnotnull(i_manager_id#1291) AND (i_manager_id#1291 = 1)) AND isnotnull(i_item_sk#1271))
                     :     :        +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_brand_id#1278,i_brand#1279,i_manager_id#1291] Batched: true, DataFilters: [isnotnull(i_manager_id#1291), (i_manager_id#1291 = 1), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,1), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_brand:string,i_manager_id:int>
                     :     +- Union
                     :        :- Project [ws_ext_sales_price#450 AS ext_price#41143, ws_item_sk#430 AS sold_item_sk#41145, ws_sold_time_sk#428 AS time_sk#41146]
                     :        :  +- BroadcastHashJoin [ws_sold_date_sk#427], [d_date_sk#24], Inner, BuildRight, false
                     :        :     :- Filter ((isnotnull(ws_sold_date_sk#427) AND isnotnull(ws_item_sk#430)) AND isnotnull(ws_sold_time_sk#428))
                     :        :     :  +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_sold_time_sk#428,ws_item_sk#430,ws_ext_sales_price#450] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#427), isnotnull(ws_item_sk#430), isnotnull(ws_sold_time_sk#428)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk), IsNotNull(ws_sold_time_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_sold_time_sk:int,ws_item_sk:int,ws_ext_sales_price:double>
                     :        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=138630]
                     :        :        +- Project [d_date_sk#24]
                     :        :           +- Filter ((((isnotnull(d_moy#32) AND isnotnull(d_year#30)) AND (d_moy#32 = 12)) AND (d_year#30 = 2000)) AND isnotnull(d_date_sk#24))
                     :        :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_moy#32), isnotnull(d_year#30), (d_moy#32 = 12), (d_year#30 = 2000), isnotnull(d_date..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,12), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                     :        :- Project [cs_ext_sales_price#484 AS ext_price#41147, cs_item_sk#476 AS sold_item_sk#41149, cs_sold_time_sk#462 AS time_sk#41150]
                     :        :  +- BroadcastHashJoin [cs_sold_date_sk#461], [d_date_sk#41158], Inner, BuildRight, false
                     :        :     :- Filter ((isnotnull(cs_sold_date_sk#461) AND isnotnull(cs_item_sk#476)) AND isnotnull(cs_sold_time_sk#462))
                     :        :     :  +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_sold_time_sk#462,cs_item_sk#476,cs_ext_sales_price#484] Batched: true, DataFilters: [isnotnull(cs_sold_date_sk#461), isnotnull(cs_item_sk#476), isnotnull(cs_sold_time_sk#462)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_item_sk), IsNotNull(cs_sold_time_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_sold_time_sk:int,cs_item_sk:int,cs_ext_sales_price:double>
                     :        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=138633]
                     :        :        +- Project [d_date_sk#41158]
                     :        :           +- Filter ((((isnotnull(d_moy#41166) AND isnotnull(d_year#41164)) AND (d_moy#41166 = 12)) AND (d_year#41164 = 2000)) AND isnotnull(d_date_sk#41158))
                     :        :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#41158,d_year#41164,d_moy#41166] Batched: true, DataFilters: [isnotnull(d_moy#41166), isnotnull(d_year#41164), (d_moy#41166 = 12), (d_year#41164 = 2000), isno..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,12), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                     :        +- Project [ss_ext_sales_price#1263 AS ext_price#41151, ss_item_sk#1250 AS sold_item_sk#41153, ss_sold_time_sk#1249 AS time_sk#41154]
                     :           +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#41186], Inner, BuildRight, false
                     :              :- Filter ((isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_item_sk#1250)) AND isnotnull(ss_sold_time_sk#1249))
                     :              :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_sold_time_sk#1249,ss_item_sk#1250,ss_ext_sales_price#1263] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250), isnotnull(ss_sold_time_sk#1249)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk), IsNotNull(ss_sold_time_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_sold_time_sk:int,ss_item_sk:int,ss_ext_sales_price:double>
                     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=138636]
                     :                 +- Project [d_date_sk#41186]
                     :                    +- Filter ((((isnotnull(d_moy#41194) AND isnotnull(d_year#41192)) AND (d_moy#41194 = 12)) AND (d_year#41192 = 2000)) AND isnotnull(d_date_sk#41186))
                     :                       +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#41186,d_year#41192,d_moy#41194] Batched: true, DataFilters: [isnotnull(d_moy#41194), isnotnull(d_year#41192), (d_moy#41194 = 12), (d_year#41192 = 2000), isno..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,12), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=138645]
                        +- Project [t_time_sk#39576, t_hour#39579, t_minute#39580]
                           +- Filter (((t_meal_time#39585 = breakfast) OR (t_meal_time#39585 = dinner)) AND isnotnull(t_time_sk#39576))
                              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#39576,t_hour#39579,t_minute#39580,t_meal_time#39585] Batched: true, DataFilters: [((t_meal_time#39585 = breakfast) OR (t_meal_time#39585 = dinner)), isnotnull(t_time_sk#39576)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [Or(EqualTo(t_meal_time,breakfast),EqualTo(t_meal_time,dinner)), IsNotNull(t_time_sk)], ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int,t_meal_time:string>
