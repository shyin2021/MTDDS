AdaptiveSparkPlan isFinalPlan=false
+- BroadcastNestedLoopJoin BuildRight, Inner
   :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :  :  :  :  :- BroadcastNestedLoopJoin BuildRight, Inner
   :  :  :  :  :  :  :- HashAggregate(keys=[], functions=[count(1)], output=[h8_30_to_9#47876L])
   :  :  :  :  :  :  :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170881]
   :  :  :  :  :  :  :     +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48422L])
   :  :  :  :  :  :  :        +- Project
   :  :  :  :  :  :  :           +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
   :  :  :  :  :  :  :              :- Project [ss_store_sk#1255]
   :  :  :  :  :  :  :              :  +- BroadcastHashJoin [ss_sold_time_sk#1249], [t_time_sk#39576], Inner, BuildRight, false
   :  :  :  :  :  :  :              :     :- Project [ss_sold_time_sk#1249, ss_store_sk#1255]
   :  :  :  :  :  :  :              :     :  +- BroadcastHashJoin [ss_hdemo_sk#1253], [hd_demo_sk#12110], Inner, BuildRight, false
   :  :  :  :  :  :  :              :     :     :- Filter ((isnotnull(ss_hdemo_sk#1253) AND isnotnull(ss_sold_time_sk#1249)) AND isnotnull(ss_store_sk#1255))
   :  :  :  :  :  :  :              :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#1249,ss_hdemo_sk#1253,ss_store_sk#1255] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#1253), isnotnull(ss_sold_time_sk#1249), isnotnull(ss_store_sk#1255)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :  :  :  :  :  :              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170868]
   :  :  :  :  :  :  :              :     :        +- Project [hd_demo_sk#12110]
   :  :  :  :  :  :  :              :     :           +- Filter (((((hd_dep_count#12113 = 3) AND (hd_vehicle_count#12114 <= 5)) OR ((hd_dep_count#12113 = -1) AND (hd_vehicle_count#12114 <= 1))) OR ((hd_dep_count#12113 = 0) AND (hd_vehicle_count#12114 <= 2))) AND isnotnull(hd_demo_sk#12110))
   :  :  :  :  :  :  :              :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#12110,hd_dep_count#12113,hd_vehicle_count#12114] Batched: true, DataFilters: [((((hd_dep_count#12113 = 3) AND (hd_vehicle_count#12114 <= 5)) OR ((hd_dep_count#12113 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :  :  :  :  :  :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170872]
   :  :  :  :  :  :  :              :        +- Project [t_time_sk#39576]
   :  :  :  :  :  :  :              :           +- Filter ((((isnotnull(t_hour#39579) AND isnotnull(t_minute#39580)) AND (t_hour#39579 = 8)) AND (t_minute#39580 >= 30)) AND isnotnull(t_time_sk#39576))
   :  :  :  :  :  :  :              :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#39576,t_hour#39579,t_minute#39580] Batched: true, DataFilters: [isnotnull(t_hour#39579), isnotnull(t_minute#39580), (t_hour#39579 = 8), (t_minute#39580 >= 30), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,8), GreaterThanOrEqual(t_minute,30), IsNo..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :  :  :  :  :  :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170876]
   :  :  :  :  :  :  :                 +- Project [s_store_sk#52]
   :  :  :  :  :  :  :                    +- Filter ((isnotnull(s_store_name#57) AND (s_store_name#57 = ese)) AND isnotnull(s_store_sk#52))
   :  :  :  :  :  :  :                       +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_store_name#57] Batched: true, DataFilters: [isnotnull(s_store_name#57), (s_store_name#57 = ese), isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  :  :  :  :  :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170899]
   :  :  :  :  :  :     +- HashAggregate(keys=[], functions=[count(1)], output=[h9_to_9_30#47877L])
   :  :  :  :  :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170896]
   :  :  :  :  :  :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48424L])
   :  :  :  :  :  :              +- Project
   :  :  :  :  :  :                 +- BroadcastHashJoin [ss_store_sk#47899], [s_store_sk#47930], Inner, BuildRight, false
   :  :  :  :  :  :                    :- Project [ss_store_sk#47899]
   :  :  :  :  :  :                    :  +- BroadcastHashJoin [ss_sold_time_sk#47893], [t_time_sk#47920], Inner, BuildRight, false
   :  :  :  :  :  :                    :     :- Project [ss_sold_time_sk#47893, ss_store_sk#47899]
   :  :  :  :  :  :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#47897], [hd_demo_sk#47915], Inner, BuildRight, false
   :  :  :  :  :  :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#47897) AND isnotnull(ss_sold_time_sk#47893)) AND isnotnull(ss_store_sk#47899))
   :  :  :  :  :  :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#47893,ss_hdemo_sk#47897,ss_store_sk#47899] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#47897), isnotnull(ss_sold_time_sk#47893), isnotnull(ss_store_sk#47899)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :  :  :  :  :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170883]
   :  :  :  :  :  :                    :     :        +- Project [hd_demo_sk#47915]
   :  :  :  :  :  :                    :     :           +- Filter (((((hd_dep_count#47918 = 3) AND (hd_vehicle_count#47919 <= 5)) OR ((hd_dep_count#47918 = -1) AND (hd_vehicle_count#47919 <= 1))) OR ((hd_dep_count#47918 = 0) AND (hd_vehicle_count#47919 <= 2))) AND isnotnull(hd_demo_sk#47915))
   :  :  :  :  :  :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#47915,hd_dep_count#47918,hd_vehicle_count#47919] Batched: true, DataFilters: [((((hd_dep_count#47918 = 3) AND (hd_vehicle_count#47919 <= 5)) OR ((hd_dep_count#47918 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :  :  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170887]
   :  :  :  :  :  :                    :        +- Project [t_time_sk#47920]
   :  :  :  :  :  :                    :           +- Filter ((((isnotnull(t_hour#47923) AND isnotnull(t_minute#47924)) AND (t_hour#47923 = 9)) AND (t_minute#47924 < 30)) AND isnotnull(t_time_sk#47920))
   :  :  :  :  :  :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#47920,t_hour#47923,t_minute#47924] Batched: true, DataFilters: [isnotnull(t_hour#47923), isnotnull(t_minute#47924), (t_hour#47923 = 9), (t_minute#47924 < 30), i..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), LessThan(t_minute,30), IsNotNull(t_ti..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :  :  :  :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170891]
   :  :  :  :  :  :                       +- Project [s_store_sk#47930]
   :  :  :  :  :  :                          +- Filter ((isnotnull(s_store_name#47935) AND (s_store_name#47935 = ese)) AND isnotnull(s_store_sk#47930))
   :  :  :  :  :  :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#47930,s_store_name#47935] Batched: true, DataFilters: [isnotnull(s_store_name#47935), (s_store_name#47935 = ese), isnotnull(s_store_sk#47930)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  :  :  :  :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170917]
   :  :  :  :  :     +- HashAggregate(keys=[], functions=[count(1)], output=[h9_30_to_10#47878L])
   :  :  :  :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170914]
   :  :  :  :  :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48426L])
   :  :  :  :  :              +- Project
   :  :  :  :  :                 +- BroadcastHashJoin [ss_store_sk#47966], [s_store_sk#47997], Inner, BuildRight, false
   :  :  :  :  :                    :- Project [ss_store_sk#47966]
   :  :  :  :  :                    :  +- BroadcastHashJoin [ss_sold_time_sk#47960], [t_time_sk#47987], Inner, BuildRight, false
   :  :  :  :  :                    :     :- Project [ss_sold_time_sk#47960, ss_store_sk#47966]
   :  :  :  :  :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#47964], [hd_demo_sk#47982], Inner, BuildRight, false
   :  :  :  :  :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#47964) AND isnotnull(ss_sold_time_sk#47960)) AND isnotnull(ss_store_sk#47966))
   :  :  :  :  :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#47960,ss_hdemo_sk#47964,ss_store_sk#47966] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#47964), isnotnull(ss_sold_time_sk#47960), isnotnull(ss_store_sk#47966)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :  :  :  :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170901]
   :  :  :  :  :                    :     :        +- Project [hd_demo_sk#47982]
   :  :  :  :  :                    :     :           +- Filter (((((hd_dep_count#47985 = 3) AND (hd_vehicle_count#47986 <= 5)) OR ((hd_dep_count#47985 = -1) AND (hd_vehicle_count#47986 <= 1))) OR ((hd_dep_count#47985 = 0) AND (hd_vehicle_count#47986 <= 2))) AND isnotnull(hd_demo_sk#47982))
   :  :  :  :  :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#47982,hd_dep_count#47985,hd_vehicle_count#47986] Batched: true, DataFilters: [((((hd_dep_count#47985 = 3) AND (hd_vehicle_count#47986 <= 5)) OR ((hd_dep_count#47985 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170905]
   :  :  :  :  :                    :        +- Project [t_time_sk#47987]
   :  :  :  :  :                    :           +- Filter ((((isnotnull(t_hour#47990) AND isnotnull(t_minute#47991)) AND (t_hour#47990 = 9)) AND (t_minute#47991 >= 30)) AND isnotnull(t_time_sk#47987))
   :  :  :  :  :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#47987,t_hour#47990,t_minute#47991] Batched: true, DataFilters: [isnotnull(t_hour#47990), isnotnull(t_minute#47991), (t_hour#47990 = 9), (t_minute#47991 >= 30), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), GreaterThanOrEqual(t_minute,30), IsNo..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :  :  :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170909]
   :  :  :  :  :                       +- Project [s_store_sk#47997]
   :  :  :  :  :                          +- Filter ((isnotnull(s_store_name#48002) AND (s_store_name#48002 = ese)) AND isnotnull(s_store_sk#47997))
   :  :  :  :  :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#47997,s_store_name#48002] Batched: true, DataFilters: [isnotnull(s_store_name#48002), (s_store_name#48002 = ese), isnotnull(s_store_sk#47997)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  :  :  :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170935]
   :  :  :  :     +- HashAggregate(keys=[], functions=[count(1)], output=[h10_to_10_30#47879L])
   :  :  :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170932]
   :  :  :  :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48428L])
   :  :  :  :              +- Project
   :  :  :  :                 +- BroadcastHashJoin [ss_store_sk#48033], [s_store_sk#48064], Inner, BuildRight, false
   :  :  :  :                    :- Project [ss_store_sk#48033]
   :  :  :  :                    :  +- BroadcastHashJoin [ss_sold_time_sk#48027], [t_time_sk#48054], Inner, BuildRight, false
   :  :  :  :                    :     :- Project [ss_sold_time_sk#48027, ss_store_sk#48033]
   :  :  :  :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#48031], [hd_demo_sk#48049], Inner, BuildRight, false
   :  :  :  :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#48031) AND isnotnull(ss_sold_time_sk#48027)) AND isnotnull(ss_store_sk#48033))
   :  :  :  :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#48027,ss_hdemo_sk#48031,ss_store_sk#48033] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#48031), isnotnull(ss_sold_time_sk#48027), isnotnull(ss_store_sk#48033)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :  :  :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170919]
   :  :  :  :                    :     :        +- Project [hd_demo_sk#48049]
   :  :  :  :                    :     :           +- Filter (((((hd_dep_count#48052 = 3) AND (hd_vehicle_count#48053 <= 5)) OR ((hd_dep_count#48052 = -1) AND (hd_vehicle_count#48053 <= 1))) OR ((hd_dep_count#48052 = 0) AND (hd_vehicle_count#48053 <= 2))) AND isnotnull(hd_demo_sk#48049))
   :  :  :  :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#48049,hd_dep_count#48052,hd_vehicle_count#48053] Batched: true, DataFilters: [((((hd_dep_count#48052 = 3) AND (hd_vehicle_count#48053 <= 5)) OR ((hd_dep_count#48052 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170923]
   :  :  :  :                    :        +- Project [t_time_sk#48054]
   :  :  :  :                    :           +- Filter ((((isnotnull(t_hour#48057) AND isnotnull(t_minute#48058)) AND (t_hour#48057 = 10)) AND (t_minute#48058 < 30)) AND isnotnull(t_time_sk#48054))
   :  :  :  :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#48054,t_hour#48057,t_minute#48058] Batched: true, DataFilters: [isnotnull(t_hour#48057), isnotnull(t_minute#48058), (t_hour#48057 = 10), (t_minute#48058 < 30), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :  :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170927]
   :  :  :  :                       +- Project [s_store_sk#48064]
   :  :  :  :                          +- Filter ((isnotnull(s_store_name#48069) AND (s_store_name#48069 = ese)) AND isnotnull(s_store_sk#48064))
   :  :  :  :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#48064,s_store_name#48069] Batched: true, DataFilters: [isnotnull(s_store_name#48069), (s_store_name#48069 = ese), isnotnull(s_store_sk#48064)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  :  :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170953]
   :  :  :     +- HashAggregate(keys=[], functions=[count(1)], output=[h10_30_to_11#47880L])
   :  :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170950]
   :  :  :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48430L])
   :  :  :              +- Project
   :  :  :                 +- BroadcastHashJoin [ss_store_sk#48100], [s_store_sk#48131], Inner, BuildRight, false
   :  :  :                    :- Project [ss_store_sk#48100]
   :  :  :                    :  +- BroadcastHashJoin [ss_sold_time_sk#48094], [t_time_sk#48121], Inner, BuildRight, false
   :  :  :                    :     :- Project [ss_sold_time_sk#48094, ss_store_sk#48100]
   :  :  :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#48098], [hd_demo_sk#48116], Inner, BuildRight, false
   :  :  :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#48098) AND isnotnull(ss_sold_time_sk#48094)) AND isnotnull(ss_store_sk#48100))
   :  :  :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#48094,ss_hdemo_sk#48098,ss_store_sk#48100] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#48098), isnotnull(ss_sold_time_sk#48094), isnotnull(ss_store_sk#48100)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :  :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170937]
   :  :  :                    :     :        +- Project [hd_demo_sk#48116]
   :  :  :                    :     :           +- Filter (((((hd_dep_count#48119 = 3) AND (hd_vehicle_count#48120 <= 5)) OR ((hd_dep_count#48119 = -1) AND (hd_vehicle_count#48120 <= 1))) OR ((hd_dep_count#48119 = 0) AND (hd_vehicle_count#48120 <= 2))) AND isnotnull(hd_demo_sk#48116))
   :  :  :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#48116,hd_dep_count#48119,hd_vehicle_count#48120] Batched: true, DataFilters: [((((hd_dep_count#48119 = 3) AND (hd_vehicle_count#48120 <= 5)) OR ((hd_dep_count#48119 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170941]
   :  :  :                    :        +- Project [t_time_sk#48121]
   :  :  :                    :           +- Filter ((((isnotnull(t_hour#48124) AND isnotnull(t_minute#48125)) AND (t_hour#48124 = 10)) AND (t_minute#48125 >= 30)) AND isnotnull(t_time_sk#48121))
   :  :  :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#48121,t_hour#48124,t_minute#48125] Batched: true, DataFilters: [isnotnull(t_hour#48124), isnotnull(t_minute#48125), (t_hour#48124 = 10), (t_minute#48125 >= 30),..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), GreaterThanOrEqual(t_minute,30), IsN..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170945]
   :  :  :                       +- Project [s_store_sk#48131]
   :  :  :                          +- Filter ((isnotnull(s_store_name#48136) AND (s_store_name#48136 = ese)) AND isnotnull(s_store_sk#48131))
   :  :  :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#48131,s_store_name#48136] Batched: true, DataFilters: [isnotnull(s_store_name#48136), (s_store_name#48136 = ese), isnotnull(s_store_sk#48131)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170971]
   :  :     +- HashAggregate(keys=[], functions=[count(1)], output=[h11_to_11_30#47881L])
   :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170968]
   :  :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48432L])
   :  :              +- Project
   :  :                 +- BroadcastHashJoin [ss_store_sk#48167], [s_store_sk#48198], Inner, BuildRight, false
   :  :                    :- Project [ss_store_sk#48167]
   :  :                    :  +- BroadcastHashJoin [ss_sold_time_sk#48161], [t_time_sk#48188], Inner, BuildRight, false
   :  :                    :     :- Project [ss_sold_time_sk#48161, ss_store_sk#48167]
   :  :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#48165], [hd_demo_sk#48183], Inner, BuildRight, false
   :  :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#48165) AND isnotnull(ss_sold_time_sk#48161)) AND isnotnull(ss_store_sk#48167))
   :  :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#48161,ss_hdemo_sk#48165,ss_store_sk#48167] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#48165), isnotnull(ss_sold_time_sk#48161), isnotnull(ss_store_sk#48167)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :  :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170955]
   :  :                    :     :        +- Project [hd_demo_sk#48183]
   :  :                    :     :           +- Filter (((((hd_dep_count#48186 = 3) AND (hd_vehicle_count#48187 <= 5)) OR ((hd_dep_count#48186 = -1) AND (hd_vehicle_count#48187 <= 1))) OR ((hd_dep_count#48186 = 0) AND (hd_vehicle_count#48187 <= 2))) AND isnotnull(hd_demo_sk#48183))
   :  :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#48183,hd_dep_count#48186,hd_vehicle_count#48187] Batched: true, DataFilters: [((((hd_dep_count#48186 = 3) AND (hd_vehicle_count#48187 <= 5)) OR ((hd_dep_count#48186 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170959]
   :  :                    :        +- Project [t_time_sk#48188]
   :  :                    :           +- Filter ((((isnotnull(t_hour#48191) AND isnotnull(t_minute#48192)) AND (t_hour#48191 = 11)) AND (t_minute#48192 < 30)) AND isnotnull(t_time_sk#48188))
   :  :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#48188,t_hour#48191,t_minute#48192] Batched: true, DataFilters: [isnotnull(t_hour#48191), isnotnull(t_minute#48192), (t_hour#48191 = 11), (t_minute#48192 < 30), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170963]
   :  :                       +- Project [s_store_sk#48198]
   :  :                          +- Filter ((isnotnull(s_store_name#48203) AND (s_store_name#48203 = ese)) AND isnotnull(s_store_sk#48198))
   :  :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#48198,s_store_name#48203] Batched: true, DataFilters: [isnotnull(s_store_name#48203), (s_store_name#48203 = ese), isnotnull(s_store_sk#48198)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   :  +- BroadcastExchange IdentityBroadcastMode, [plan_id=170989]
   :     +- HashAggregate(keys=[], functions=[count(1)], output=[h11_30_to_12#47882L])
   :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=170986]
   :           +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48434L])
   :              +- Project
   :                 +- BroadcastHashJoin [ss_store_sk#48234], [s_store_sk#48265], Inner, BuildRight, false
   :                    :- Project [ss_store_sk#48234]
   :                    :  +- BroadcastHashJoin [ss_sold_time_sk#48228], [t_time_sk#48255], Inner, BuildRight, false
   :                    :     :- Project [ss_sold_time_sk#48228, ss_store_sk#48234]
   :                    :     :  +- BroadcastHashJoin [ss_hdemo_sk#48232], [hd_demo_sk#48250], Inner, BuildRight, false
   :                    :     :     :- Filter ((isnotnull(ss_hdemo_sk#48232) AND isnotnull(ss_sold_time_sk#48228)) AND isnotnull(ss_store_sk#48234))
   :                    :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#48228,ss_hdemo_sk#48232,ss_store_sk#48234] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#48232), isnotnull(ss_sold_time_sk#48228), isnotnull(ss_store_sk#48234)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
   :                    :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170973]
   :                    :     :        +- Project [hd_demo_sk#48250]
   :                    :     :           +- Filter (((((hd_dep_count#48253 = 3) AND (hd_vehicle_count#48254 <= 5)) OR ((hd_dep_count#48253 = -1) AND (hd_vehicle_count#48254 <= 1))) OR ((hd_dep_count#48253 = 0) AND (hd_vehicle_count#48254 <= 2))) AND isnotnull(hd_demo_sk#48250))
   :                    :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#48250,hd_dep_count#48253,hd_vehicle_count#48254] Batched: true, DataFilters: [((((hd_dep_count#48253 = 3) AND (hd_vehicle_count#48254 <= 5)) OR ((hd_dep_count#48253 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
   :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170977]
   :                    :        +- Project [t_time_sk#48255]
   :                    :           +- Filter ((((isnotnull(t_hour#48258) AND isnotnull(t_minute#48259)) AND (t_hour#48258 = 11)) AND (t_minute#48259 >= 30)) AND isnotnull(t_time_sk#48255))
   :                    :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#48255,t_hour#48258,t_minute#48259] Batched: true, DataFilters: [isnotnull(t_hour#48258), isnotnull(t_minute#48259), (t_hour#48258 = 11), (t_minute#48259 >= 30),..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), GreaterThanOrEqual(t_minute,30), IsN..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
   :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170981]
   :                       +- Project [s_store_sk#48265]
   :                          +- Filter ((isnotnull(s_store_name#48270) AND (s_store_name#48270 = ese)) AND isnotnull(s_store_sk#48265))
   :                             +- FileScan parquet spark_catalog.m.store[s_store_sk#48265,s_store_name#48270] Batched: true, DataFilters: [isnotnull(s_store_name#48270), (s_store_name#48270 = ese), isnotnull(s_store_sk#48265)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
   +- BroadcastExchange IdentityBroadcastMode, [plan_id=171007]
      +- HashAggregate(keys=[], functions=[count(1)], output=[h12_to_12_30#47883L])
         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=171004]
            +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#48436L])
               +- Project
                  +- BroadcastHashJoin [ss_store_sk#48301], [s_store_sk#48332], Inner, BuildRight, false
                     :- Project [ss_store_sk#48301]
                     :  +- BroadcastHashJoin [ss_sold_time_sk#48295], [t_time_sk#48322], Inner, BuildRight, false
                     :     :- Project [ss_sold_time_sk#48295, ss_store_sk#48301]
                     :     :  +- BroadcastHashJoin [ss_hdemo_sk#48299], [hd_demo_sk#48317], Inner, BuildRight, false
                     :     :     :- Filter ((isnotnull(ss_hdemo_sk#48299) AND isnotnull(ss_sold_time_sk#48295)) AND isnotnull(ss_store_sk#48301))
                     :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_time_sk#48295,ss_hdemo_sk#48299,ss_store_sk#48301] Batched: true, DataFilters: [isnotnull(ss_hdemo_sk#48299), isnotnull(ss_sold_time_sk#48295), isnotnull(ss_store_sk#48301)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>
                     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170991]
                     :     :        +- Project [hd_demo_sk#48317]
                     :     :           +- Filter (((((hd_dep_count#48320 = 3) AND (hd_vehicle_count#48321 <= 5)) OR ((hd_dep_count#48320 = -1) AND (hd_vehicle_count#48321 <= 1))) OR ((hd_dep_count#48320 = 0) AND (hd_vehicle_count#48321 <= 2))) AND isnotnull(hd_demo_sk#48317))
                     :     :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#48317,hd_dep_count#48320,hd_vehicle_count#48321] Batched: true, DataFilters: [((((hd_dep_count#48320 = 3) AND (hd_vehicle_count#48321 <= 5)) OR ((hd_dep_count#48320 = -1) AND..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,3),LessThanOrEqual(hd_vehicle_count,5)),And(EqualTo(hd_dep_count,..., ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170995]
                     :        +- Project [t_time_sk#48322]
                     :           +- Filter ((((isnotnull(t_hour#48325) AND isnotnull(t_minute#48326)) AND (t_hour#48325 = 12)) AND (t_minute#48326 < 30)) AND isnotnull(t_time_sk#48322))
                     :              +- FileScan parquet spark_catalog.m.time_dim[t_time_sk#48322,t_hour#48325,t_minute#48326] Batched: true, DataFilters: [isnotnull(t_hour#48325), isnotnull(t_minute#48326), (t_hour#48325 = 12), (t_minute#48326 < 30), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/time_dim], PartitionFilters: [], PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,12), LessThan(t_minute,30), IsNotNull(t_t..., ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=170999]
                        +- Project [s_store_sk#48332]
                           +- Filter ((isnotnull(s_store_name#48337) AND (s_store_name#48337 = ese)) AND isnotnull(s_store_sk#48332))
                              +- FileScan parquet spark_catalog.m.store[s_store_sk#48332,s_store_name#48337] Batched: true, DataFilters: [isnotnull(s_store_name#48337), (s_store_name#48337 = ese), isnotnull(s_store_sk#48332)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_name:string>
