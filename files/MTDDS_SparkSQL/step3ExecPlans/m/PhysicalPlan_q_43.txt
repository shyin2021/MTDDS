AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[s_store_name#57 ASC NULLS FIRST,s_store_id#53 ASC NULLS FIRST,sun_sales#31365 ASC NULLS FIRST,mon_sales#31366 ASC NULLS FIRST,tue_sales#31367 ASC NULLS FIRST,wed_sales#31368 ASC NULLS FIRST,thu_sales#31369 ASC NULLS FIRST,fri_sales#31370 ASC NULLS FIRST,sat_sales#31371 ASC NULLS FIRST], output=[s_store_name#57,s_store_id#53,sun_sales#31365,mon_sales#31366,tue_sales#31367,wed_sales#31368,thu_sales#31369,fri_sales#31370,sat_sales#31371])
   +- HashAggregate(keys=[s_store_name#57, s_store_id#53], functions=[sum(CASE WHEN (d_day_name#38 = Sunday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Monday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Tuesday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Wednesday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Thursday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Friday) THEN ss_sales_price#1261 END), sum(CASE WHEN (d_day_name#38 = Saturday) THEN ss_sales_price#1261 END)], output=[s_store_name#57, s_store_id#53, sun_sales#31365, mon_sales#31366, tue_sales#31367, wed_sales#31368, thu_sales#31369, fri_sales#31370, sat_sales#31371])
      +- Exchange hashpartitioning(s_store_name#57, s_store_id#53, 200), ENSURE_REQUIREMENTS, [plan_id=92572]
         +- HashAggregate(keys=[s_store_name#57, s_store_id#53], functions=[partial_sum(CASE WHEN (d_day_name#38 = Sunday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Monday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Tuesday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Wednesday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Thursday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Friday) THEN ss_sales_price#1261 END), partial_sum(CASE WHEN (d_day_name#38 = Saturday) THEN ss_sales_price#1261 END)], output=[s_store_name#57, s_store_id#53, sum#31467, sum#31468, sum#31469, sum#31470, sum#31471, sum#31472, sum#31473])
            +- Project [d_day_name#38, ss_sales_price#1261, s_store_id#53, s_store_name#57]
               +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
                  :- Project [d_day_name#38, ss_store_sk#1255, ss_sales_price#1261]
                  :  +- BroadcastHashJoin [d_date_sk#24], [ss_sold_date_sk#1248], Inner, BuildLeft, false
                  :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=92563]
                  :     :  +- Project [d_date_sk#24, d_day_name#38]
                  :     :     +- Filter ((isnotnull(d_year#30) AND (d_year#30 = 2000)) AND isnotnull(d_date_sk#24))
                  :     :        +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_day_name#38] Batched: true, DataFilters: [isnotnull(d_year#30), (d_year#30 = 2000), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_day_name:string>
                  :     +- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_store_sk#1255))
                  :        +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_store_sk#1255,ss_sales_price#1261] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_store_sk#1255)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_store_sk:int,ss_sales_price:double>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=92567]
                     +- Project [s_store_sk#52, s_store_id#53, s_store_name#57]
                        +- Filter ((isnotnull(s_gmt_offset#79) AND (s_gmt_offset#79 = -6.0)) AND isnotnull(s_store_sk#52))
                           +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_store_id#53,s_store_name#57,s_gmt_offset#79] Batched: true, DataFilters: [isnotnull(s_gmt_offset#79), (s_gmt_offset#79 = -6.0), isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_gmt_offset), EqualTo(s_gmt_offset,-6.0), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_id:string,s_store_name:string,s_gmt_offset:double>
