AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[sum(ss_quantity#1258)], output=[sum(ss_quantity)#33116L])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=98454]
      +- HashAggregate(keys=[], functions=[partial_sum(ss_quantity#1258)], output=[sum#33122L])
         +- Project [ss_quantity#1258]
            +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
               :- Project [ss_sold_date_sk#1248, ss_quantity#1258]
               :  +- BroadcastHashJoin [ss_addr_sk#1254], [ca_address_sk#8171], Inner, BuildRight, ((((ca_state#8179 IN (ID,IA,CA) AND (ss_net_profit#1270 >= 0.0)) AND (ss_net_profit#1270 <= 2000.0)) OR ((ca_state#8179 IN (IN,WI,IL) AND (ss_net_profit#1270 >= 150.0)) AND (ss_net_profit#1270 <= 3000.0))) OR ((ca_state#8179 IN (TX,WA,MN) AND (ss_net_profit#1270 >= 50.0)) AND (ss_net_profit#1270 <= 25000.0))), false
               :     :- Project [ss_sold_date_sk#1248, ss_addr_sk#1254, ss_quantity#1258, ss_net_profit#1270]
               :     :  +- BroadcastHashJoin [ss_cdemo_sk#1252], [cd_demo_sk#8266], Inner, BuildRight, ((((((cd_marital_status#8268 = M) AND (cd_education_status#8269 = Unknown)) AND (ss_sales_price#1261 >= 100.0)) AND (ss_sales_price#1261 <= 150.0)) OR ((((cd_marital_status#8268 = S) AND (cd_education_status#8269 = 4 yr Degree)) AND (ss_sales_price#1261 >= 50.0)) AND (ss_sales_price#1261 <= 100.0))) OR ((((cd_marital_status#8268 = D) AND (cd_education_status#8269 = 2 yr Degree)) AND (ss_sales_price#1261 >= 150.0)) AND (ss_sales_price#1261 <= 200.0))), false
               :     :     :- Project [ss_sold_date_sk#1248, ss_cdemo_sk#1252, ss_addr_sk#1254, ss_quantity#1258, ss_sales_price#1261, ss_net_profit#1270]
               :     :     :  +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
               :     :     :     :- Filter (((((isnotnull(ss_store_sk#1255) AND isnotnull(ss_cdemo_sk#1252)) AND isnotnull(ss_addr_sk#1254)) AND isnotnull(ss_sold_date_sk#1248)) AND ((((ss_sales_price#1261 >= 100.0) AND (ss_sales_price#1261 <= 150.0)) OR ((ss_sales_price#1261 >= 50.0) AND (ss_sales_price#1261 <= 100.0))) OR ((ss_sales_price#1261 >= 150.0) AND (ss_sales_price#1261 <= 200.0)))) AND ((((ss_net_profit#1270 >= 0.0) AND (ss_net_profit#1270 <= 2000.0)) OR ((ss_net_profit#1270 >= 150.0) AND (ss_net_profit#1270 <= 3000.0))) OR ((ss_net_profit#1270 >= 50.0) AND (ss_net_profit#1270 <= 25000.0))))
               :     :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_cdemo_sk#1252,ss_addr_sk#1254,ss_store_sk#1255,ss_quantity#1258,ss_sales_price#1261,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_store_sk#1255), isnotnull(ss_cdemo_sk#1252), isnotnull(ss_addr_sk#1254), isnotnull(..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), IsNotNull(ss_cdemo_sk), IsNotNull(ss_addr_sk), IsNotNull(ss_sold_date_sk..., ReadSchema: struct<ss_sold_date_sk:int,ss_cdemo_sk:int,ss_addr_sk:int,ss_store_sk:int,ss_quantity:int,ss_sale...
               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=98437]
               :     :     :        +- Filter isnotnull(s_store_sk#52)
               :     :     :           +- FileScan parquet spark_catalog.m.store[s_store_sk#52] Batched: true, DataFilters: [isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int>
               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=98441]
               :     :        +- Filter (isnotnull(cd_demo_sk#8266) AND ((((cd_marital_status#8268 = M) AND (cd_education_status#8269 = Unknown)) OR ((cd_marital_status#8268 = S) AND (cd_education_status#8269 = 4 yr Degree))) OR ((cd_marital_status#8268 = D) AND (cd_education_status#8269 = 2 yr Degree))))
               :     :           +- FileScan parquet spark_catalog.m.customer_demographics[cd_demo_sk#8266,cd_marital_status#8268,cd_education_status#8269] Batched: true, DataFilters: [isnotnull(cd_demo_sk#8266), ((((cd_marital_status#8268 = M) AND (cd_education_status#8269 = Unkn..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_demograph..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk), Or(Or(And(EqualTo(cd_marital_status,M),EqualTo(cd_education_status,Unknow..., ReadSchema: struct<cd_demo_sk:int,cd_marital_status:string,cd_education_status:string>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=98445]
               :        +- Project [ca_address_sk#8171, ca_state#8179]
               :           +- Filter (((isnotnull(ca_country#8181) AND (ca_country#8181 = United States)) AND isnotnull(ca_address_sk#8171)) AND ((ca_state#8179 IN (ID,IA,CA) OR ca_state#8179 IN (IN,WI,IL)) OR ca_state#8179 IN (TX,WA,MN)))
               :              +- FileScan parquet spark_catalog.m.customer_address[ca_address_sk#8171,ca_state#8179,ca_country#8181] Batched: true, DataFilters: [isnotnull(ca_country#8181), (ca_country#8181 = United States), isnotnull(ca_address_sk#8171), ((..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_country), EqualTo(ca_country,United States), IsNotNull(ca_address_sk), Or(Or(In(ca_..., ReadSchema: struct<ca_address_sk:int,ca_state:string,ca_country:string>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=98449]
                  +- Project [d_date_sk#24]
                     +- Filter ((isnotnull(d_year#30) AND (d_year#30 = 1998)) AND isnotnull(d_date_sk#24))
                        +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30] Batched: true, DataFilters: [isnotnull(d_year#30), (d_year#30 = 1998), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1998), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
