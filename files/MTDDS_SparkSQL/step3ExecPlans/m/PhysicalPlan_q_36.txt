AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[lochierarchy#29706 DESC NULLS LAST,CASE WHEN ((cast((shiftright(spark_grouping_id#29716L, 1) & 1) as tinyint) + cast((shiftright(spark_grouping_id#29716L, 0) & 1) as tinyint)) = 0) THEN i_category#29717 END ASC NULLS FIRST,rank_within_parent#29707 ASC NULLS FIRST], output=[gross_margin#29705,i_category#29717,i_class#29718,lochierarchy#29706,rank_within_parent#29707])
   +- Project [gross_margin#29705, i_category#29717, i_class#29718, lochierarchy#29706, rank_within_parent#29707, spark_grouping_id#29716L]
      +- Window [rank(_w0#29727) windowspecdefinition(_w1#29731, _w2#29732, _w0#29727 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#29707], [_w1#29731, _w2#29732], [_w0#29727 ASC NULLS FIRST]
         +- Sort [_w1#29731 ASC NULLS FIRST, _w2#29732 ASC NULLS FIRST, _w0#29727 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(_w1#29731, _w2#29732, 200), ENSURE_REQUIREMENTS, [plan_id=85868]
               +- HashAggregate(keys=[i_category#29717, i_class#29718, spark_grouping_id#29716L], functions=[sum(ss_net_profit#1270), sum(ss_ext_sales_price#1263)], output=[gross_margin#29705, i_category#29717, i_class#29718, lochierarchy#29706, _w0#29727, _w1#29731, _w2#29732, spark_grouping_id#29716L])
                  +- Exchange hashpartitioning(i_category#29717, i_class#29718, spark_grouping_id#29716L, 200), ENSURE_REQUIREMENTS, [plan_id=85865]
                     +- HashAggregate(keys=[i_category#29717, i_class#29718, spark_grouping_id#29716L], functions=[partial_sum(ss_net_profit#1270), partial_sum(ss_ext_sales_price#1263)], output=[i_category#29717, i_class#29718, spark_grouping_id#29716L, sum#29760, sum#29761])
                        +- Expand [[ss_ext_sales_price#1263, ss_net_profit#1270, i_category#1283, i_class#1281, 0], [ss_ext_sales_price#1263, ss_net_profit#1270, i_category#1283, null, 1], [ss_ext_sales_price#1263, ss_net_profit#1270, null, null, 3]], [ss_ext_sales_price#1263, ss_net_profit#1270, i_category#29717, i_class#29718, spark_grouping_id#29716L]
                           +- Project [ss_ext_sales_price#1263, ss_net_profit#1270, i_category#1283, i_class#1281]
                              +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
                                 :- Project [ss_store_sk#1255, ss_ext_sales_price#1263, ss_net_profit#1270, i_class#1281, i_category#1283]
                                 :  +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                                 :     :- Project [ss_item_sk#1250, ss_store_sk#1255, ss_ext_sales_price#1263, ss_net_profit#1270]
                                 :     :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
                                 :     :     :- Filter ((isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_item_sk#1250)) AND isnotnull(ss_store_sk#1255))
                                 :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_store_sk#1255,ss_ext_sales_price#1263,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250), isnotnull(ss_store_sk#1255)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_store_sk:int,ss_ext_sales_price:double,ss_net_profit...
                                 :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=85851]
                                 :     :        +- Project [d_date_sk#24]
                                 :     :           +- Filter ((isnotnull(d_year#30) AND (d_year#30 = 1998)) AND isnotnull(d_date_sk#24))
                                 :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30] Batched: true, DataFilters: [isnotnull(d_year#30), (d_year#30 = 1998), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1998), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
                                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=85855]
                                 :        +- Filter isnotnull(i_item_sk#1271)
                                 :           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_class#1281,i_category#1283] Batched: true, DataFilters: [isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_class:string,i_category:string>
                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=85859]
                                    +- Project [s_store_sk#52]
                                       +- Filter (s_state#76 IN (TN,AL,SD) AND isnotnull(s_store_sk#52))
                                          +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_state#76] Batched: true, DataFilters: [s_state#76 IN (TN,AL,SD), isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [In(s_state, [AL,SD,TN]), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_state:string>
