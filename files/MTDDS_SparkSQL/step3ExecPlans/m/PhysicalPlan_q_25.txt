AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[i_item_id#1272 ASC NULLS FIRST,i_item_desc#1275 ASC NULLS FIRST,s_store_id#53 ASC NULLS FIRST,s_store_name#57 ASC NULLS FIRST], output=[i_item_id#1272,i_item_desc#1275,s_store_id#53,s_store_name#57,store_sales_profit#25153,store_returns_loss#25154,catalog_sales_profit#25155])
   +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, s_store_id#53, s_store_name#57], functions=[stddev_samp(ss_net_profit#1270), stddev_samp(sr_net_loss#23), stddev_samp(cs_net_profit#494)], output=[i_item_id#1272, i_item_desc#1275, s_store_id#53, s_store_name#57, store_sales_profit#25153, store_returns_loss#25154, catalog_sales_profit#25155])
      +- Exchange hashpartitioning(i_item_id#1272, i_item_desc#1275, s_store_id#53, s_store_name#57, 200), ENSURE_REQUIREMENTS, [plan_id=69888]
         +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, s_store_id#53, s_store_name#57], functions=[partial_stddev_samp(ss_net_profit#1270), partial_stddev_samp(sr_net_loss#23), partial_stddev_samp(cs_net_profit#494)], output=[i_item_id#1272, i_item_desc#1275, s_store_id#53, s_store_name#57, n#25217, avg#25218, m2#25219, n#25226, avg#25227, m2#25228, n#25235, avg#25236, m2#25237])
            +- Project [ss_net_profit#1270, sr_net_loss#23, cs_net_profit#494, s_store_id#53, s_store_name#57, i_item_id#1272, i_item_desc#1275]
               +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                  :- Project [ss_item_sk#1250, ss_net_profit#1270, sr_net_loss#23, cs_net_profit#494, s_store_id#53, s_store_name#57]
                  :  +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
                  :     :- Project [ss_item_sk#1250, ss_store_sk#1255, ss_net_profit#1270, sr_net_loss#23, cs_net_profit#494]
                  :     :  +- BroadcastHashJoin [cs_sold_date_sk#461], [d_date_sk#25184], Inner, BuildRight, false
                  :     :     :- Project [ss_item_sk#1250, ss_store_sk#1255, ss_net_profit#1270, sr_net_loss#23, cs_sold_date_sk#461, cs_net_profit#494]
                  :     :     :  +- BroadcastHashJoin [sr_returned_date_sk#4], [d_date_sk#25156], Inner, BuildRight, false
                  :     :     :     :- Project [ss_item_sk#1250, ss_store_sk#1255, ss_net_profit#1270, sr_returned_date_sk#4, sr_net_loss#23, cs_sold_date_sk#461, cs_net_profit#494]
                  :     :     :     :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
                  :     :     :     :     :- Project [ss_sold_date_sk#1248, ss_item_sk#1250, ss_store_sk#1255, ss_net_profit#1270, sr_returned_date_sk#4, sr_net_loss#23, cs_sold_date_sk#461, cs_net_profit#494]
                  :     :     :     :     :  +- SortMergeJoin [sr_customer_sk#7, sr_item_sk#6], [cs_bill_customer_sk#464, cs_item_sk#476], Inner
                  :     :     :     :     :     :- Sort [sr_customer_sk#7 ASC NULLS FIRST, sr_item_sk#6 ASC NULLS FIRST], false, 0
                  :     :     :     :     :     :  +- Exchange hashpartitioning(sr_customer_sk#7, sr_item_sk#6, 200), ENSURE_REQUIREMENTS, [plan_id=69860]
                  :     :     :     :     :     :     +- Project [ss_sold_date_sk#1248, ss_item_sk#1250, ss_store_sk#1255, ss_net_profit#1270, sr_returned_date_sk#4, sr_item_sk#6, sr_customer_sk#7, sr_net_loss#23]
                  :     :     :     :     :     :        +- BroadcastHashJoin [ss_customer_sk#1251, ss_item_sk#1250, ss_ticket_number#1257], [sr_customer_sk#7, sr_item_sk#6, sr_ticket_number#13], Inner, BuildRight, false
                  :     :     :     :     :     :           :- Filter ((((isnotnull(ss_customer_sk#1251) AND isnotnull(ss_item_sk#1250)) AND isnotnull(ss_ticket_number#1257)) AND isnotnull(ss_sold_date_sk#1248)) AND isnotnull(ss_store_sk#1255))
                  :     :     :     :     :     :           :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_customer_sk#1251,ss_store_sk#1255,ss_ticket_number#1257,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_customer_sk#1251), isnotnull(ss_item_sk#1250), isnotnull(ss_ticket_number#1257), is..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_customer_sk), IsNotNull(ss_item_sk), IsNotNull(ss_ticket_number), IsNotNull(ss_sold..., ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_store_sk:int,ss_ticket_number:int...
                  :     :     :     :     :     :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, int, false], input[1, int, false], input[3, int, false]),false), [plan_id=69855]
                  :     :     :     :     :     :              +- Filter (((isnotnull(sr_customer_sk#7) AND isnotnull(sr_item_sk#6)) AND isnotnull(sr_ticket_number#13)) AND isnotnull(sr_returned_date_sk#4))
                  :     :     :     :     :     :                 +- FileScan parquet spark_catalog.m.store_returns[sr_returned_date_sk#4,sr_item_sk#6,sr_customer_sk#7,sr_ticket_number#13,sr_net_loss#23] Batched: true, DataFilters: [isnotnull(sr_customer_sk#7), isnotnull(sr_item_sk#6), isnotnull(sr_ticket_number#13), isnotnull(..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_customer_sk), IsNotNull(sr_item_sk), IsNotNull(sr_ticket_number), IsNotNull(sr_retu..., ReadSchema: struct<sr_returned_date_sk:int,sr_item_sk:int,sr_customer_sk:int,sr_ticket_number:int,sr_net_loss...
                  :     :     :     :     :     +- Sort [cs_bill_customer_sk#464 ASC NULLS FIRST, cs_item_sk#476 ASC NULLS FIRST], false, 0
                  :     :     :     :     :        +- Exchange hashpartitioning(cs_bill_customer_sk#464, cs_item_sk#476, 200), ENSURE_REQUIREMENTS, [plan_id=69861]
                  :     :     :     :     :           +- Filter ((isnotnull(cs_bill_customer_sk#464) AND isnotnull(cs_item_sk#476)) AND isnotnull(cs_sold_date_sk#461))
                  :     :     :     :     :              +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_bill_customer_sk#464,cs_item_sk#476,cs_net_profit#494] Batched: true, DataFilters: [isnotnull(cs_bill_customer_sk#464), isnotnull(cs_item_sk#476), isnotnull(cs_sold_date_sk#461)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_bill_customer_sk), IsNotNull(cs_item_sk), IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int,cs_item_sk:int,cs_net_profit:double>
                  :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=69867]
                  :     :     :     :        +- Project [d_date_sk#24]
                  :     :     :     :           +- Filter ((((isnotnull(d_moy#32) AND isnotnull(d_year#30)) AND (d_moy#32 = 4)) AND (d_year#30 = 2002)) AND isnotnull(d_date_sk#24))
                  :     :     :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_moy#32), isnotnull(d_year#30), (d_moy#32 = 4), (d_year#30 = 2002), isnotnull(d_date_..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,4), EqualTo(d_year,2002), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=69871]
                  :     :     :        +- Project [d_date_sk#25156]
                  :     :     :           +- Filter (((((isnotnull(d_moy#25164) AND isnotnull(d_year#25162)) AND (d_moy#25164 >= 4)) AND (d_moy#25164 <= 10)) AND (d_year#25162 = 2002)) AND isnotnull(d_date_sk#25156))
                  :     :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#25156,d_year#25162,d_moy#25164] Batched: true, DataFilters: [isnotnull(d_moy#25164), isnotnull(d_year#25162), (d_moy#25164 >= 4), (d_moy#25164 <= 10), (d_yea..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), GreaterThanOrEqual(d_moy,4), LessThanOrEqual(d_moy,10), Equ..., ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=69875]
                  :     :        +- Project [d_date_sk#25184]
                  :     :           +- Filter (((((isnotnull(d_moy#25192) AND isnotnull(d_year#25190)) AND (d_moy#25192 >= 4)) AND (d_moy#25192 <= 10)) AND (d_year#25190 = 2002)) AND isnotnull(d_date_sk#25184))
                  :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#25184,d_year#25190,d_moy#25192] Batched: true, DataFilters: [isnotnull(d_moy#25192), isnotnull(d_year#25190), (d_moy#25192 >= 4), (d_moy#25192 <= 10), (d_yea..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), GreaterThanOrEqual(d_moy,4), LessThanOrEqual(d_moy,10), Equ..., ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=69879]
                  :        +- Filter isnotnull(s_store_sk#52)
                  :           +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_store_id#53,s_store_name#57] Batched: true, DataFilters: [isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_id:string,s_store_name:string>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=69883]
                     +- Filter isnotnull(i_item_sk#1271)
                        +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_item_id#1272,i_item_desc#1275] Batched: true, DataFilters: [isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string>
