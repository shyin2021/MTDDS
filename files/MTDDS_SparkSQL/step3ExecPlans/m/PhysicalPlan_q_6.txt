AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[cnt#8166L ASC NULLS FIRST,ca_state#8179 ASC NULLS FIRST], output=[state#8165,cnt#8166L])
   +- Filter (cnt#8166L >= 10)
      +- HashAggregate(keys=[ca_state#8179], functions=[count(1)], output=[state#8165, cnt#8166L, ca_state#8179])
         +- Exchange hashpartitioning(ca_state#8179, 200), ENSURE_REQUIREMENTS, [plan_id=9579]
            +- HashAggregate(keys=[ca_state#8179], functions=[partial_count(1)], output=[ca_state#8179, count#8246L])
               +- Project [ca_state#8179]
                  +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                     :- Project [ca_state#8179, ss_item_sk#1250]
                     :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
                     :     :- Project [ca_state#8179, ss_sold_date_sk#1248, ss_item_sk#1250]
                     :     :  +- SortMergeJoin [c_customer_sk#81], [ss_customer_sk#1251], Inner
                     :     :     :- Sort [c_customer_sk#81 ASC NULLS FIRST], false, 0
                     :     :     :  +- Exchange hashpartitioning(c_customer_sk#81, 200), ENSURE_REQUIREMENTS, [plan_id=9556]
                     :     :     :     +- Project [ca_state#8179, c_customer_sk#81]
                     :     :     :        +- BroadcastHashJoin [ca_address_sk#8171], [c_current_addr_sk#85], Inner, BuildLeft, false
                     :     :     :           :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=9551]
                     :     :     :           :  +- Filter isnotnull(ca_address_sk#8171)
                     :     :     :           :     +- FileScan parquet spark_catalog.m.customer_address[ca_address_sk#8171,ca_state#8179] Batched: true, DataFilters: [isnotnull(ca_address_sk#8171)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_state:string>
                     :     :     :           +- Filter (isnotnull(c_current_addr_sk#85) AND isnotnull(c_customer_sk#81))
                     :     :     :              +- FileScan parquet spark_catalog.m.customer[c_customer_sk#81,c_current_addr_sk#85] Batched: true, DataFilters: [isnotnull(c_current_addr_sk#85), isnotnull(c_customer_sk#81)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_current_addr_sk), IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_current_addr_sk:int>
                     :     :     +- Sort [ss_customer_sk#1251 ASC NULLS FIRST], false, 0
                     :     :        +- Exchange hashpartitioning(ss_customer_sk#1251, 200), ENSURE_REQUIREMENTS, [plan_id=9557]
                     :     :           +- Filter ((isnotnull(ss_customer_sk#1251) AND isnotnull(ss_sold_date_sk#1248)) AND isnotnull(ss_item_sk#1250))
                     :     :              +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_customer_sk#1251] Batched: true, DataFilters: [isnotnull(ss_customer_sk#1251), isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_customer_sk), IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int>
                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9563]
                     :        +- Project [d_date_sk#24]
                     :           +- Filter ((isnotnull(d_month_seq#27) AND (d_month_seq#27 = Subquery subquery#8167, [id=#9522])) AND isnotnull(d_date_sk#24))
                     :              :  +- Subquery subquery#8167, [id=#9522]
                     :              :     +- AdaptiveSparkPlan isFinalPlan=false
                     :              :        +- HashAggregate(keys=[d_month_seq#8190], functions=[], output=[d_month_seq#8190])
                     :              :           +- Exchange hashpartitioning(d_month_seq#8190, 200), ENSURE_REQUIREMENTS, [plan_id=9520]
                     :              :              +- HashAggregate(keys=[d_month_seq#8190], functions=[], output=[d_month_seq#8190])
                     :              :                 +- Project [d_month_seq#8190]
                     :              :                    +- Filter (((isnotnull(d_year#8193) AND isnotnull(d_moy#8195)) AND (d_year#8193 = 2002)) AND (d_moy#8195 = 3))
                     :              :                       +- FileScan parquet spark_catalog.m.date_dim[d_month_seq#8190,d_year#8193,d_moy#8195] Batched: true, DataFilters: [isnotnull(d_year#8193), isnotnull(d_moy#8195), (d_year#8193 = 2002), (d_moy#8195 = 3)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2002), EqualTo(d_moy,3)], ReadSchema: struct<d_month_seq:int,d_year:int,d_moy:int>
                     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_month_seq#27] Batched: true, DataFilters: [isnotnull(d_month_seq#27), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9574]
                        +- Project [i_item_sk#1271]
                           +- BroadcastHashJoin [i_category#1283], [i_category#8227], Inner, BuildRight, (i_current_price#1276 > (1.2 * avg(i_current_price)#8185)), false
                              :- Filter ((isnotnull(i_current_price#1276) AND isnotnull(i_category#1283)) AND isnotnull(i_item_sk#1271))
                              :  +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_current_price#1276,i_category#1283] Batched: true, DataFilters: [isnotnull(i_current_price#1276), isnotnull(i_category#1283), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_current_price), IsNotNull(i_category), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_current_price:double,i_category:string>
                              +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [plan_id=9570]
                                 +- Filter isnotnull(avg(i_current_price)#8185)
                                    +- HashAggregate(keys=[i_category#8227], functions=[avg(i_current_price#8220)], output=[avg(i_current_price)#8185, i_category#8227])
                                       +- Exchange hashpartitioning(i_category#8227, 200), ENSURE_REQUIREMENTS, [plan_id=9566]
                                          +- HashAggregate(keys=[i_category#8227], functions=[partial_avg(i_current_price#8220)], output=[i_category#8227, sum#8249, count#8250L])
                                             +- Filter isnotnull(i_category#8227)
                                                +- FileScan parquet spark_catalog.m.item[i_current_price#8220,i_category#8227] Batched: true, DataFilters: [isnotnull(i_category#8227)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_category)], ReadSchema: struct<i_current_price:double,i_category:string>
