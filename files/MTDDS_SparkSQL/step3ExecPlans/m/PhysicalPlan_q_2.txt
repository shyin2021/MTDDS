AdaptiveSparkPlan isFinalPlan=false
+- Sort [d_week_seq1#400 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(d_week_seq1#400 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=2161]
      +- Project [d_week_seq1#400, round((sun_sales1#401 / sun_sales2#409), 2) AS round((sun_sales1 / sun_sales2), 2)#568, round((mon_sales1#402 / mon_sales2#410), 2) AS round((mon_sales1 / mon_sales2), 2)#569, round((tue_sales1#403 / tue_sales2#411), 2) AS round((tue_sales1 / tue_sales2), 2)#570, round((wed_sales1#404 / wed_sales2#412), 2) AS round((wed_sales1 / wed_sales2), 2)#571, round((thu_sales1#405 / thu_sales2#413), 2) AS round((thu_sales1 / thu_sales2), 2)#572, round((fri_sales1#406 / fri_sales2#414), 2) AS round((fri_sales1 / fri_sales2), 2)#573, round((sat_sales1#407 / sat_sales2#415), 2) AS round((sat_sales1 / sat_sales2), 2)#574]
         +- SortMergeJoin [d_week_seq1#400], [(d_week_seq2#408 - 53)], Inner
            :- Sort [d_week_seq1#400 ASC NULLS FIRST], false, 0
            :  +- Project [d_week_seq#28 AS d_week_seq1#400, sun_sales#420 AS sun_sales1#401, mon_sales#421 AS mon_sales1#402, tue_sales#422 AS tue_sales1#403, wed_sales#423 AS wed_sales1#404, thu_sales#424 AS thu_sales1#405, fri_sales#425 AS fri_sales1#406, sat_sales#426 AS sat_sales1#407]
            :     +- BroadcastHashJoin [d_week_seq#28], [d_week_seq#499], Inner, BuildRight, false
            :        :- HashAggregate(keys=[d_week_seq#28], functions=[sum(CASE WHEN (d_day_name#38 = Sunday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Monday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Tuesday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Wednesday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Thursday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Friday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#38 = Saturday) THEN sales_price#417 END)], output=[d_week_seq#28, sun_sales#420, mon_sales#421, tue_sales#422, wed_sales#423, thu_sales#424, fri_sales#425, sat_sales#426])
            :        :  +- Exchange hashpartitioning(d_week_seq#28, 200), ENSURE_REQUIREMENTS, [plan_id=2136]
            :        :     +- HashAggregate(keys=[d_week_seq#28], functions=[partial_sum(CASE WHEN (d_day_name#38 = Sunday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Monday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Tuesday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Wednesday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Thursday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Friday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#38 = Saturday) THEN sales_price#417 END)], output=[d_week_seq#28, sum#1224, sum#1225, sum#1226, sum#1227, sum#1228, sum#1229, sum#1230])
            :        :        +- Project [sales_price#417, d_week_seq#28, d_day_name#38]
            :        :           +- BroadcastHashJoin [sold_date_sk#416], [d_date_sk#24], Inner, BuildRight, false
            :        :              :- Union
            :        :              :  :- Project [ws_sold_date_sk#427 AS sold_date_sk#416, ws_ext_sales_price#450 AS sales_price#417]
            :        :              :  :  +- Filter isnotnull(ws_sold_date_sk#427)
            :        :              :  :     +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_ext_sales_price#450] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#427)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_ext_sales_price:double>
            :        :              :  +- Project [cs_sold_date_sk#461 AS sold_date_sk#418, cs_ext_sales_price#484 AS sales_price#419]
            :        :              :     +- Filter isnotnull(cs_sold_date_sk#461)
            :        :              :        +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_ext_sales_price#484] Batched: true, DataFilters: [isnotnull(cs_sold_date_sk#461)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_ext_sales_price:double>
            :        :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2131]
            :        :                 +- Filter (isnotnull(d_date_sk#24) AND isnotnull(d_week_seq#28))
            :        :                    +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_week_seq#28,d_day_name#38] Batched: true, DataFilters: [isnotnull(d_date_sk#24), isnotnull(d_week_seq#28)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>
            :        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2139]
            :           +- Project [d_week_seq#499]
            :              +- Filter ((isnotnull(d_year#501) AND (d_year#501 = 1998)) AND isnotnull(d_week_seq#499))
            :                 +- FileScan parquet spark_catalog.m.date_dim[d_week_seq#499,d_year#501] Batched: true, DataFilters: [isnotnull(d_year#501), (d_year#501 = 1998), isnotnull(d_week_seq#499)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1998), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>
            +- Sort [(d_week_seq2#408 - 53) ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning((d_week_seq2#408 - 53), 200), ENSURE_REQUIREMENTS, [plan_id=2155]
                  +- Project [d_week_seq#1186 AS d_week_seq2#408, sun_sales#561 AS sun_sales2#409, mon_sales#562 AS mon_sales2#410, tue_sales#563 AS tue_sales2#411, wed_sales#564 AS wed_sales2#412, thu_sales#565 AS thu_sales2#413, fri_sales#566 AS fri_sales2#414, sat_sales#567 AS sat_sales2#415]
                     +- BroadcastHashJoin [d_week_seq#1186], [d_week_seq#527], Inner, BuildRight, false
                        :- HashAggregate(keys=[d_week_seq#1186], functions=[sum(CASE WHEN (d_day_name#1196 = Sunday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Monday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Tuesday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Wednesday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Thursday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Friday) THEN sales_price#417 END), sum(CASE WHEN (d_day_name#1196 = Saturday) THEN sales_price#417 END)], output=[d_week_seq#1186, sun_sales#561, mon_sales#562, tue_sales#563, wed_sales#564, thu_sales#565, fri_sales#566, sat_sales#567])
                        :  +- Exchange hashpartitioning(d_week_seq#1186, 200), ENSURE_REQUIREMENTS, [plan_id=2147]
                        :     +- HashAggregate(keys=[d_week_seq#1186], functions=[partial_sum(CASE WHEN (d_day_name#1196 = Sunday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Monday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Tuesday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Wednesday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Thursday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Friday) THEN sales_price#417 END), partial_sum(CASE WHEN (d_day_name#1196 = Saturday) THEN sales_price#417 END)], output=[d_week_seq#1186, sum#1238, sum#1239, sum#1240, sum#1241, sum#1242, sum#1243, sum#1244])
                        :        +- Project [sales_price#417, d_week_seq#1186, d_day_name#1196]
                        :           +- BroadcastHashJoin [sold_date_sk#416], [d_date_sk#1182], Inner, BuildRight, false
                        :              :- Union
                        :              :  :- Project [ws_sold_date_sk#1114 AS sold_date_sk#416, ws_ext_sales_price#1137 AS sales_price#417]
                        :              :  :  +- Filter isnotnull(ws_sold_date_sk#1114)
                        :              :  :     +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#1114,ws_ext_sales_price#1137] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#1114)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_ext_sales_price:double>
                        :              :  +- Project [cs_sold_date_sk#1148 AS sold_date_sk#418, cs_ext_sales_price#1171 AS sales_price#419]
                        :              :     +- Filter isnotnull(cs_sold_date_sk#1148)
                        :              :        +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#1148,cs_ext_sales_price#1171] Batched: true, DataFilters: [isnotnull(cs_sold_date_sk#1148)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_ext_sales_price:double>
                        :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2142]
                        :                 +- Filter (isnotnull(d_date_sk#1182) AND isnotnull(d_week_seq#1186))
                        :                    +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#1182,d_week_seq#1186,d_day_name#1196] Batched: true, DataFilters: [isnotnull(d_date_sk#1182), isnotnull(d_week_seq#1186)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>
                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2150]
                           +- Project [d_week_seq#527]
                              +- Filter ((isnotnull(d_year#529) AND (d_year#529 = 1999)) AND isnotnull(d_week_seq#527))
                                 +- FileScan parquet spark_catalog.m.date_dim[d_week_seq#527,d_year#529] Batched: true, DataFilters: [isnotnull(d_year#529), (d_year#529 = 1999), isnotnull(d_week_seq#527)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1999), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>
