AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[count(1)], output=[count(1)#29887L])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=87949]
      +- HashAggregate(keys=[], functions=[partial_count(1)], output=[count#29897L])
         +- Project
            +- SortMergeJoin [coalesce(c_last_name#90, ), isnull(c_last_name#90), coalesce(c_first_name#89, ), isnull(c_first_name#89), coalesce(d_date#26, ), isnull(d_date#26)], [coalesce(c_last_name#29878, ), isnull(c_last_name#29878), coalesce(c_first_name#29877, ), isnull(c_first_name#29877), coalesce(d_date#29843, ), isnull(d_date#29843)], LeftSemi
               :- SortMergeJoin [coalesce(c_last_name#90, ), isnull(c_last_name#90), coalesce(c_first_name#89, ), isnull(c_first_name#89), coalesce(d_date#26, ), isnull(d_date#26)], [coalesce(c_last_name#29832, ), isnull(c_last_name#29832), coalesce(c_first_name#29831, ), isnull(c_first_name#29831), coalesce(d_date#29797, ), isnull(d_date#29797)], LeftSemi
               :  :- Sort [coalesce(c_last_name#90, ) ASC NULLS FIRST, isnull(c_last_name#90) ASC NULLS FIRST, coalesce(c_first_name#89, ) ASC NULLS FIRST, isnull(c_first_name#89) ASC NULLS FIRST, coalesce(d_date#26, ) ASC NULLS FIRST, isnull(d_date#26) ASC NULLS FIRST], false, 0
               :  :  +- Exchange hashpartitioning(coalesce(c_last_name#90, ), isnull(c_last_name#90), coalesce(c_first_name#89, ), isnull(c_first_name#89), coalesce(d_date#26, ), isnull(d_date#26), 200), ENSURE_REQUIREMENTS, [plan_id=87925]
               :  :     +- HashAggregate(keys=[c_last_name#90, c_first_name#89, d_date#26], functions=[], output=[c_last_name#90, c_first_name#89, d_date#26])
               :  :        +- Exchange hashpartitioning(c_last_name#90, c_first_name#89, d_date#26, 200), ENSURE_REQUIREMENTS, [plan_id=87910]
               :  :           +- HashAggregate(keys=[c_last_name#90, c_first_name#89, d_date#26], functions=[], output=[c_last_name#90, c_first_name#89, d_date#26])
               :  :              +- Project [c_last_name#90, c_first_name#89, d_date#26]
               :  :                 +- BroadcastHashJoin [ss_customer_sk#1251], [c_customer_sk#81], Inner, BuildRight, false
               :  :                    :- Project [ss_customer_sk#1251, d_date#26]
               :  :                    :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
               :  :                    :     :- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_customer_sk#1251))
               :  :                    :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_customer_sk#1251] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_customer_sk#1251)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_customer_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_customer_sk:int>
               :  :                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=87901]
               :  :                    :        +- Project [d_date_sk#24, d_date#26]
               :  :                    :           +- Filter (((isnotnull(d_month_seq#27) AND (d_month_seq#27 >= 1179)) AND (d_month_seq#27 <= 1190)) AND isnotnull(d_date_sk#24))
               :  :                    :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_date#26,d_month_seq#27] Batched: true, DataFilters: [isnotnull(d_month_seq#27), (d_month_seq#27 >= 1179), (d_month_seq#27 <= 1190), isnotnull(d_date_..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1179), LessThanOrEqual(d_month_seq,1190),..., ReadSchema: struct<d_date_sk:int,d_date:string,d_month_seq:int>
               :  :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=87905]
               :  :                       +- Filter isnotnull(c_customer_sk#81)
               :  :                          +- FileScan parquet spark_catalog.m.customer[c_customer_sk#81,c_first_name#89,c_last_name#90] Batched: true, DataFilters: [isnotnull(c_customer_sk#81)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_first_name:string,c_last_name:string>
               :  +- Sort [coalesce(c_last_name#29832, ) ASC NULLS FIRST, isnull(c_last_name#29832) ASC NULLS FIRST, coalesce(c_first_name#29831, ) ASC NULLS FIRST, isnull(c_first_name#29831) ASC NULLS FIRST, coalesce(d_date#29797, ) ASC NULLS FIRST, isnull(d_date#29797) ASC NULLS FIRST], false, 0
               :     +- Exchange hashpartitioning(coalesce(c_last_name#29832, ), isnull(c_last_name#29832), coalesce(c_first_name#29831, ), isnull(c_first_name#29831), coalesce(d_date#29797, ), isnull(d_date#29797), 200), ENSURE_REQUIREMENTS, [plan_id=87926]
               :        +- HashAggregate(keys=[c_last_name#29832, c_first_name#29831, d_date#29797], functions=[], output=[c_last_name#29832, c_first_name#29831, d_date#29797])
               :           +- Exchange hashpartitioning(c_last_name#29832, c_first_name#29831, d_date#29797, 200), ENSURE_REQUIREMENTS, [plan_id=87921]
               :              +- HashAggregate(keys=[c_last_name#29832, c_first_name#29831, d_date#29797], functions=[], output=[c_last_name#29832, c_first_name#29831, d_date#29797])
               :                 +- Project [c_last_name#29832, c_first_name#29831, d_date#29797]
               :                    +- BroadcastHashJoin [cs_bill_customer_sk#464], [c_customer_sk#29823], Inner, BuildRight, false
               :                       :- Project [cs_bill_customer_sk#464, d_date#29797]
               :                       :  +- BroadcastHashJoin [cs_sold_date_sk#461], [d_date_sk#29795], Inner, BuildRight, false
               :                       :     :- Filter (isnotnull(cs_sold_date_sk#461) AND isnotnull(cs_bill_customer_sk#464))
               :                       :     :  +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_bill_customer_sk#464] Batched: true, DataFilters: [isnotnull(cs_sold_date_sk#461), isnotnull(cs_bill_customer_sk#464)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_bill_customer_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_bill_customer_sk:int>
               :                       :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=87912]
               :                       :        +- Project [d_date_sk#29795, d_date#29797]
               :                       :           +- Filter (((isnotnull(d_month_seq#29798) AND (d_month_seq#29798 >= 1179)) AND (d_month_seq#29798 <= 1190)) AND isnotnull(d_date_sk#29795))
               :                       :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#29795,d_date#29797,d_month_seq#29798] Batched: true, DataFilters: [isnotnull(d_month_seq#29798), (d_month_seq#29798 >= 1179), (d_month_seq#29798 <= 1190), isnotnul..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1179), LessThanOrEqual(d_month_seq,1190),..., ReadSchema: struct<d_date_sk:int,d_date:string,d_month_seq:int>
               :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=87916]
               :                          +- Filter isnotnull(c_customer_sk#29823)
               :                             +- FileScan parquet spark_catalog.m.customer[c_customer_sk#29823,c_first_name#29831,c_last_name#29832] Batched: true, DataFilters: [isnotnull(c_customer_sk#29823)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_first_name:string,c_last_name:string>
               +- Sort [coalesce(c_last_name#29878, ) ASC NULLS FIRST, isnull(c_last_name#29878) ASC NULLS FIRST, coalesce(c_first_name#29877, ) ASC NULLS FIRST, isnull(c_first_name#29877) ASC NULLS FIRST, coalesce(d_date#29843, ) ASC NULLS FIRST, isnull(d_date#29843) ASC NULLS FIRST], false, 0
                  +- Exchange hashpartitioning(coalesce(c_last_name#29878, ), isnull(c_last_name#29878), coalesce(c_first_name#29877, ), isnull(c_first_name#29877), coalesce(d_date#29843, ), isnull(d_date#29843), 200), ENSURE_REQUIREMENTS, [plan_id=87943]
                     +- HashAggregate(keys=[c_last_name#29878, c_first_name#29877, d_date#29843], functions=[], output=[c_last_name#29878, c_first_name#29877, d_date#29843])
                        +- Exchange hashpartitioning(c_last_name#29878, c_first_name#29877, d_date#29843, 200), ENSURE_REQUIREMENTS, [plan_id=87939]
                           +- HashAggregate(keys=[c_last_name#29878, c_first_name#29877, d_date#29843], functions=[], output=[c_last_name#29878, c_first_name#29877, d_date#29843])
                              +- Project [c_last_name#29878, c_first_name#29877, d_date#29843]
                                 +- BroadcastHashJoin [ws_bill_customer_sk#431], [c_customer_sk#29869], Inner, BuildRight, false
                                    :- Project [ws_bill_customer_sk#431, d_date#29843]
                                    :  +- BroadcastHashJoin [ws_sold_date_sk#427], [d_date_sk#29841], Inner, BuildRight, false
                                    :     :- Filter (isnotnull(ws_sold_date_sk#427) AND isnotnull(ws_bill_customer_sk#431))
                                    :     :  +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_bill_customer_sk#431] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#427), isnotnull(ws_bill_customer_sk#431)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_bill_customer_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_bill_customer_sk:int>
                                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=87930]
                                    :        +- Project [d_date_sk#29841, d_date#29843]
                                    :           +- Filter (((isnotnull(d_month_seq#29844) AND (d_month_seq#29844 >= 1179)) AND (d_month_seq#29844 <= 1190)) AND isnotnull(d_date_sk#29841))
                                    :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#29841,d_date#29843,d_month_seq#29844] Batched: true, DataFilters: [isnotnull(d_month_seq#29844), (d_month_seq#29844 >= 1179), (d_month_seq#29844 <= 1190), isnotnul..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1179), LessThanOrEqual(d_month_seq,1190),..., ReadSchema: struct<d_date_sk:int,d_date:string,d_month_seq:int>
                                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=87934]
                                       +- Filter isnotnull(c_customer_sk#29869)
                                          +- FileScan parquet spark_catalog.m.customer[c_customer_sk#29869,c_first_name#29877,c_last_name#29878] Batched: true, DataFilters: [isnotnull(c_customer_sk#29869)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_first_name:string,c_last_name:string>
