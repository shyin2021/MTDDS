AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[rnk#31478 ASC NULLS FIRST], output=[rnk#31478,best_performing#31484,worst_performing#31485])
   +- Project [rnk#31478, i_product_name#1292 AS best_performing#31484, i_product_name#31534 AS worst_performing#31485]
      +- BroadcastHashJoin [item_sk#31479], [i_item_sk#31513], Inner, BuildRight, false
         :- Project [rnk#31478, item_sk#31479, i_product_name#1292]
         :  +- BroadcastHashJoin [item_sk#31474], [i_item_sk#1271], Inner, BuildRight, false
         :     :- Project [item_sk#31474, rnk#31478, item_sk#31479]
         :     :  +- SortMergeJoin [rnk#31478], [rnk#31483], Inner
         :     :     :- Sort [rnk#31478 ASC NULLS FIRST], false, 0
         :     :     :  +- Project [item_sk#31474, rnk#31478]
         :     :     :     +- Filter ((rnk#31478 < 11) AND isnotnull(item_sk#31474))
         :     :     :        +- Window [rank(rank_col#31475) windowspecdefinition(rank_col#31475 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rnk#31478], [rank_col#31475 ASC NULLS FIRST]
         :     :     :           +- WindowGroupLimit [rank_col#31475 ASC NULLS FIRST], rank(rank_col#31475), 10, Final
         :     :     :              +- Sort [rank_col#31475 ASC NULLS FIRST], false, 0
         :     :     :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=93440]
         :     :     :                    +- WindowGroupLimit [rank_col#31475 ASC NULLS FIRST], rank(rank_col#31475), 10, Partial
         :     :     :                       +- Sort [rank_col#31475 ASC NULLS FIRST], false, 0
         :     :     :                          +- Filter (isnotnull(rank_col#31475) AND (rank_col#31475 > (0.9 * Subquery subquery#31477, [id=#93406])))
         :     :     :                             :  +- Subquery subquery#31477, [id=#93406]
         :     :     :                             :     +- AdaptiveSparkPlan isFinalPlan=false
         :     :     :                             :        +- HashAggregate(keys=[ss_store_sk#31547], functions=[avg(ss_net_profit#31562)], output=[rank_col#31476])
         :     :     :                             :           +- Exchange hashpartitioning(ss_store_sk#31547, 200), ENSURE_REQUIREMENTS, [plan_id=93392]
         :     :     :                             :              +- HashAggregate(keys=[ss_store_sk#31547], functions=[partial_avg(ss_net_profit#31562)], output=[ss_store_sk#31547, sum#31623, count#31624L])
         :     :     :                             :                 +- Project [ss_store_sk#31547, ss_net_profit#31562]
         :     :     :                             :                    +- Filter ((isnotnull(ss_store_sk#31547) AND (ss_store_sk#31547 = 29)) AND isnull(ss_cdemo_sk#31544))
         :     :     :                             :                       +- FileScan parquet spark_catalog.m.store_sales[ss_cdemo_sk#31544,ss_store_sk#31547,ss_net_profit#31562] Batched: true, DataFilters: [isnotnull(ss_store_sk#31547), (ss_store_sk#31547 = 29), isnull(ss_cdemo_sk#31544)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,29), IsNull(ss_cdemo_sk)], ReadSchema: struct<ss_cdemo_sk:int,ss_store_sk:int,ss_net_profit:double>
         :     :     :                             +- HashAggregate(keys=[ss_item_sk#1250], functions=[avg(ss_net_profit#1270)], output=[item_sk#31474, rank_col#31475])
         :     :     :                                +- Exchange hashpartitioning(ss_item_sk#1250, 200), ENSURE_REQUIREMENTS, [plan_id=93433]
         :     :     :                                   +- HashAggregate(keys=[ss_item_sk#1250], functions=[partial_avg(ss_net_profit#1270)], output=[ss_item_sk#1250, sum#31615, count#31616L])
         :     :     :                                      +- Project [ss_item_sk#1250, ss_net_profit#1270]
         :     :     :                                         +- Filter (isnotnull(ss_store_sk#1255) AND (ss_store_sk#1255 = 29))
         :     :     :                                            +- FileScan parquet spark_catalog.m.store_sales[ss_item_sk#1250,ss_store_sk#1255,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_store_sk#1255), (ss_store_sk#1255 = 29)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,29)], ReadSchema: struct<ss_item_sk:int,ss_store_sk:int,ss_net_profit:double>
         :     :     +- Sort [rnk#31483 ASC NULLS FIRST], false, 0
         :     :        +- Project [item_sk#31479, rnk#31483]
         :     :           +- Filter ((rnk#31483 < 11) AND isnotnull(item_sk#31479))
         :     :              +- Window [rank(rank_col#31480) windowspecdefinition(rank_col#31480 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rnk#31483], [rank_col#31480 DESC NULLS LAST]
         :     :                 +- WindowGroupLimit [rank_col#31480 DESC NULLS LAST], rank(rank_col#31480), 10, Final
         :     :                    +- Sort [rank_col#31480 DESC NULLS LAST], false, 0
         :     :                       +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=93453]
         :     :                          +- WindowGroupLimit [rank_col#31480 DESC NULLS LAST], rank(rank_col#31480), 10, Partial
         :     :                             +- Sort [rank_col#31480 DESC NULLS LAST], false, 0
         :     :                                +- Filter (isnotnull(rank_col#31480) AND (rank_col#31480 > (0.9 * Subquery subquery#31482, [id=#93413])))
         :     :                                   :  +- Subquery subquery#31482, [id=#93413]
         :     :                                   :     +- AdaptiveSparkPlan isFinalPlan=false
         :     :                                   :        +- HashAggregate(keys=[ss_store_sk#31547], functions=[avg(ss_net_profit#31562)], output=[rank_col#31476])
         :     :                                   :           +- Exchange hashpartitioning(ss_store_sk#31547, 200), ENSURE_REQUIREMENTS, [plan_id=93404]
         :     :                                   :              +- HashAggregate(keys=[ss_store_sk#31547], functions=[partial_avg(ss_net_profit#31562)], output=[ss_store_sk#31547, sum#31623, count#31624L])
         :     :                                   :                 +- Project [ss_store_sk#31547, ss_net_profit#31562]
         :     :                                   :                    +- Filter ((isnotnull(ss_store_sk#31547) AND (ss_store_sk#31547 = 29)) AND isnull(ss_cdemo_sk#31544))
         :     :                                   :                       +- FileScan parquet spark_catalog.m.store_sales[ss_cdemo_sk#31544,ss_store_sk#31547,ss_net_profit#31562] Batched: true, DataFilters: [isnotnull(ss_store_sk#31547), (ss_store_sk#31547 = 29), isnull(ss_cdemo_sk#31544)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,29), IsNull(ss_cdemo_sk)], ReadSchema: struct<ss_cdemo_sk:int,ss_store_sk:int,ss_net_profit:double>
         :     :                                   +- HashAggregate(keys=[ss_item_sk#31492], functions=[avg(ss_net_profit#31512)], output=[item_sk#31479, rank_col#31480])
         :     :                                      +- Exchange hashpartitioning(ss_item_sk#31492, 200), ENSURE_REQUIREMENTS, [plan_id=93446]
         :     :                                         +- HashAggregate(keys=[ss_item_sk#31492], functions=[partial_avg(ss_net_profit#31512)], output=[ss_item_sk#31492, sum#31619, count#31620L])
         :     :                                            +- Project [ss_item_sk#31492, ss_net_profit#31512]
         :     :                                               +- Filter (isnotnull(ss_store_sk#31497) AND (ss_store_sk#31497 = 29))
         :     :                                                  +- FileScan parquet spark_catalog.m.store_sales[ss_item_sk#31492,ss_store_sk#31497,ss_net_profit#31512] Batched: true, DataFilters: [isnotnull(ss_store_sk#31497), (ss_store_sk#31497 = 29)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_store_sk), EqualTo(ss_store_sk,29)], ReadSchema: struct<ss_item_sk:int,ss_store_sk:int,ss_net_profit:double>
         :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=93466]
         :        +- Filter isnotnull(i_item_sk#1271)
         :           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_product_name#1292] Batched: true, DataFilters: [isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_product_name:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=93470]
            +- Filter isnotnull(i_item_sk#31513)
               +- FileScan parquet spark_catalog.m.item[i_item_sk#31513,i_product_name#31534] Batched: true, DataFilters: [isnotnull(i_item_sk#31513)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_product_name:string>
