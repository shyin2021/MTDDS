AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[c_last_name#90 ASC NULLS FIRST,c_first_name#89 ASC NULLS FIRST,ca_city#31792 ASC NULLS FIRST,bought_city#31783 ASC NULLS FIRST,ss_ticket_number#1257 ASC NULLS FIRST], output=[c_last_name#90,c_first_name#89,ca_city#31792,bought_city#31783,ss_ticket_number#1257,amt#31784,profit#31785])
   +- Project [c_last_name#90, c_first_name#89, ca_city#31792, bought_city#31783, ss_ticket_number#1257, amt#31784, profit#31785]
      +- BroadcastHashJoin [c_current_addr_sk#85], [ca_address_sk#31786], Inner, BuildRight, NOT (ca_city#31792 = bought_city#31783), false
         :- Project [ss_ticket_number#1257, bought_city#31783, amt#31784, profit#31785, c_current_addr_sk#85, c_first_name#89, c_last_name#90]
         :  +- BroadcastHashJoin [ss_customer_sk#1251], [c_customer_sk#81], Inner, BuildRight, false
         :     :- HashAggregate(keys=[ss_ticket_number#1257, ss_customer_sk#1251, ss_addr_sk#1254, ca_city#8177], functions=[sum(ss_coupon_amt#1267), sum(ss_net_profit#1270)], output=[ss_ticket_number#1257, ss_customer_sk#1251, bought_city#31783, amt#31784, profit#31785])
         :     :  +- Exchange hashpartitioning(ss_ticket_number#1257, ss_customer_sk#1251, ss_addr_sk#1254, ca_city#8177, 200), ENSURE_REQUIREMENTS, [plan_id=95321]
         :     :     +- HashAggregate(keys=[ss_ticket_number#1257, ss_customer_sk#1251, ss_addr_sk#1254, ca_city#8177], functions=[partial_sum(ss_coupon_amt#1267), partial_sum(ss_net_profit#1270)], output=[ss_ticket_number#1257, ss_customer_sk#1251, ss_addr_sk#1254, ca_city#8177, sum#31832, sum#31833])
         :     :        +- Project [ss_customer_sk#1251, ss_addr_sk#1254, ss_ticket_number#1257, ss_coupon_amt#1267, ss_net_profit#1270, ca_city#8177]
         :     :           +- BroadcastHashJoin [ss_addr_sk#1254], [ca_address_sk#8171], Inner, BuildRight, false
         :     :              :- Project [ss_customer_sk#1251, ss_addr_sk#1254, ss_ticket_number#1257, ss_coupon_amt#1267, ss_net_profit#1270]
         :     :              :  +- BroadcastHashJoin [ss_hdemo_sk#1253], [hd_demo_sk#12110], Inner, BuildRight, false
         :     :              :     :- Project [ss_customer_sk#1251, ss_hdemo_sk#1253, ss_addr_sk#1254, ss_ticket_number#1257, ss_coupon_amt#1267, ss_net_profit#1270]
         :     :              :     :  +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
         :     :              :     :     :- Project [ss_customer_sk#1251, ss_hdemo_sk#1253, ss_addr_sk#1254, ss_store_sk#1255, ss_ticket_number#1257, ss_coupon_amt#1267, ss_net_profit#1270]
         :     :              :     :     :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
         :     :              :     :     :     :- Filter ((((isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_store_sk#1255)) AND isnotnull(ss_hdemo_sk#1253)) AND isnotnull(ss_addr_sk#1254)) AND isnotnull(ss_customer_sk#1251))
         :     :              :     :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_customer_sk#1251,ss_hdemo_sk#1253,ss_addr_sk#1254,ss_store_sk#1255,ss_ticket_number#1257,ss_coupon_amt#1267,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_store_sk#1255), isnotnull(ss_hdemo_sk#1253), isnot..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk), IsNotNull(ss_hdemo_sk), IsNotNull(ss_addr_sk..., ReadSchema: struct<ss_sold_date_sk:int,ss_customer_sk:int,ss_hdemo_sk:int,ss_addr_sk:int,ss_store_sk:int,ss_t...
         :     :              :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=95304]
         :     :              :     :     :        +- Project [d_date_sk#24]
         :     :              :     :     :           +- Filter ((d_dow#31 IN (6,0) AND d_year#30 IN (1999,2000,2001)) AND isnotnull(d_date_sk#24))
         :     :              :     :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_dow#31] Batched: true, DataFilters: [d_dow#31 IN (6,0), d_year#30 IN (1999,2000,2001), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [In(d_dow, [0,6]), In(d_year, [1999,2000,2001]), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_dow:int>
         :     :              :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=95308]
         :     :              :     :        +- Project [s_store_sk#52]
         :     :              :     :           +- Filter (s_city#74 IN (Five Points,Riverside,Fairview,Midway,Pleasant Hill) AND isnotnull(s_store_sk#52))
         :     :              :     :              +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_city#74] Batched: true, DataFilters: [s_city#74 IN (Five Points,Riverside,Fairview,Midway,Pleasant Hill), isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [In(s_city, [Fairview,Five Points,Midway,Pleasant Hill,Riverside]), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_city:string>
         :     :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=95312]
         :     :              :        +- Project [hd_demo_sk#12110]
         :     :              :           +- Filter (((hd_dep_count#12113 = 8) OR (hd_vehicle_count#12114 = 4)) AND isnotnull(hd_demo_sk#12110))
         :     :              :              +- FileScan parquet spark_catalog.m.household_demographics[hd_demo_sk#12110,hd_dep_count#12113,hd_vehicle_count#12114] Batched: true, DataFilters: [((hd_dep_count#12113 = 8) OR (hd_vehicle_count#12114 = 4)), isnotnull(hd_demo_sk#12110)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/household_demograp..., PartitionFilters: [], PushedFilters: [Or(EqualTo(hd_dep_count,8),EqualTo(hd_vehicle_count,4)), IsNotNull(hd_demo_sk)], ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>
         :     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=95316]
         :     :                 +- Filter (isnotnull(ca_address_sk#8171) AND isnotnull(ca_city#8177))
         :     :                    +- FileScan parquet spark_catalog.m.customer_address[ca_address_sk#8171,ca_city#8177] Batched: true, DataFilters: [isnotnull(ca_address_sk#8171), isnotnull(ca_city#8177)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk), IsNotNull(ca_city)], ReadSchema: struct<ca_address_sk:int,ca_city:string>
         :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=95324]
         :        +- Filter (isnotnull(c_customer_sk#81) AND isnotnull(c_current_addr_sk#85))
         :           +- FileScan parquet spark_catalog.m.customer[c_customer_sk#81,c_current_addr_sk#85,c_first_name#89,c_last_name#90] Batched: true, DataFilters: [isnotnull(c_customer_sk#81), isnotnull(c_current_addr_sk#85)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer], PartitionFilters: [], PushedFilters: [IsNotNull(c_customer_sk), IsNotNull(c_current_addr_sk)], ReadSchema: struct<c_customer_sk:int,c_current_addr_sk:int,c_first_name:string,c_last_name:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=95328]
            +- Filter (isnotnull(ca_address_sk#31786) AND isnotnull(ca_city#31792))
               +- FileScan parquet spark_catalog.m.customer_address[ca_address_sk#31786,ca_city#31792] Batched: true, DataFilters: [isnotnull(ca_address_sk#31786), isnotnull(ca_city#31792)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk), IsNotNull(ca_city)], ReadSchema: struct<ca_address_sk:int,ca_city:string>
