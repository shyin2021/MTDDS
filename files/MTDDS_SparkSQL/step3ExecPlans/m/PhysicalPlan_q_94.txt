AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[], functions=[sum(ws_ext_ship_cost#455), sum(ws_net_profit#460), count(distinct ws_order_number#444)], output=[order_count#48728L, total_shipping_cost#48729, total_net_profit#48730])
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=175629]
      +- HashAggregate(keys=[], functions=[merge_sum(ws_ext_ship_cost#455), merge_sum(ws_net_profit#460), partial_count(distinct ws_order_number#444)], output=[sum#48784, sum#48786, count#48789L])
         +- HashAggregate(keys=[ws_order_number#444], functions=[merge_sum(ws_ext_ship_cost#455), merge_sum(ws_net_profit#460)], output=[ws_order_number#444, sum#48784, sum#48786])
            +- Exchange hashpartitioning(ws_order_number#444, 200), ENSURE_REQUIREMENTS, [plan_id=175625]
               +- HashAggregate(keys=[ws_order_number#444], functions=[partial_sum(ws_ext_ship_cost#455), partial_sum(ws_net_profit#460)], output=[ws_order_number#444, sum#48784, sum#48786])
                  +- Project [ws_order_number#444, ws_ext_ship_cost#455, ws_net_profit#460]
                     +- BroadcastHashJoin [ws_web_site_sk#440], [web_site_sk#7871], Inner, BuildRight, false
                        :- Project [ws_web_site_sk#440, ws_order_number#444, ws_ext_ship_cost#455, ws_net_profit#460]
                        :  +- BroadcastHashJoin [ws_ship_addr_sk#438], [ca_address_sk#8171], Inner, BuildRight, false
                        :     :- Project [ws_ship_addr_sk#438, ws_web_site_sk#440, ws_order_number#444, ws_ext_ship_cost#455, ws_net_profit#460]
                        :     :  +- BroadcastHashJoin [ws_ship_date_sk#429], [d_date_sk#24], Inner, BuildRight, false
                        :     :     :- BroadcastHashJoin [ws_order_number#444], [wr_order_number#7860], LeftAnti, BuildRight, false
                        :     :     :  :- Project [ws_ship_date_sk#429, ws_ship_addr_sk#438, ws_web_site_sk#440, ws_order_number#444, ws_ext_ship_cost#455, ws_net_profit#460]
                        :     :     :  :  +- BroadcastHashJoin [ws_order_number#444], [ws_order_number#48750], LeftSemi, BuildRight, NOT (ws_warehouse_sk#442 = ws_warehouse_sk#48748), false
                        :     :     :  :     :- Filter ((isnotnull(ws_ship_date_sk#429) AND isnotnull(ws_ship_addr_sk#438)) AND isnotnull(ws_web_site_sk#440))
                        :     :     :  :     :  +- FileScan parquet spark_catalog.m.web_sales[ws_ship_date_sk#429,ws_ship_addr_sk#438,ws_web_site_sk#440,ws_warehouse_sk#442,ws_order_number#444,ws_ext_ship_cost#455,ws_net_profit#460] Batched: true, DataFilters: [isnotnull(ws_ship_date_sk#429), isnotnull(ws_ship_addr_sk#438), isnotnull(ws_web_site_sk#440)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_ship_date_sk), IsNotNull(ws_ship_addr_sk), IsNotNull(ws_web_site_sk)], ReadSchema: struct<ws_ship_date_sk:int,ws_ship_addr_sk:int,ws_web_site_sk:int,ws_warehouse_sk:int,ws_order_nu...
                        :     :     :  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=175605]
                        :     :     :  :        +- FileScan parquet spark_catalog.m.web_sales[ws_warehouse_sk#48748,ws_order_number#48750] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ws_warehouse_sk:int,ws_order_number:int>
                        :     :     :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=175609]
                        :     :     :     +- FileScan parquet spark_catalog.m.web_returns[wr_order_number#7860] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_returns], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<wr_order_number:int>
                        :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=175612]
                        :     :        +- Project [d_date_sk#24]
                        :     :           +- Filter (((isnotnull(d_date#26) AND (d_date#26 >= 1999-5-01)) AND (cast(d_date#26 as date) <= 1999-06-30)) AND isnotnull(d_date_sk#24))
                        :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_date#26] Batched: true, DataFilters: [isnotnull(d_date#26), (d_date#26 >= 1999-5-01), (cast(d_date#26 as date) <= 1999-06-30), isnotnu..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), GreaterThanOrEqual(d_date,1999-5-01), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
                        :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=175616]
                        :        +- Project [ca_address_sk#8171]
                        :           +- Filter ((isnotnull(ca_state#8179) AND (ca_state#8179 = ND)) AND isnotnull(ca_address_sk#8171))
                        :              +- FileScan parquet spark_catalog.m.customer_address[ca_address_sk#8171,ca_state#8179] Batched: true, DataFilters: [isnotnull(ca_state#8179), (ca_state#8179 = ND), isnotnull(ca_address_sk#8171)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_address], PartitionFilters: [], PushedFilters: [IsNotNull(ca_state), EqualTo(ca_state,ND), IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_state:string>
                        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=175620]
                           +- Project [web_site_sk#7871]
                              +- Filter ((isnotnull(web_company_name#7885) AND (web_company_name#7885 = pri)) AND isnotnull(web_site_sk#7871))
                                 +- FileScan parquet spark_catalog.m.web_site[web_site_sk#7871,web_company_name#7885] Batched: true, DataFilters: [isnotnull(web_company_name#7885), (web_company_name#7885 = pri), isnotnull(web_site_sk#7871)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_site], PartitionFilters: [], PushedFilters: [IsNotNull(web_company_name), EqualTo(web_company_name,pri), IsNotNull(web_site_sk)], ReadSchema: struct<web_site_sk:int,web_company_name:string>
