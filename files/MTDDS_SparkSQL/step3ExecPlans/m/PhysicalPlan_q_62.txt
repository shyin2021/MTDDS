AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[substr(w_warehouse_name, 1, 20)#37126 ASC NULLS FIRST,sm_type#37117 ASC NULLS FIRST,web_name#7875 ASC NULLS FIRST], output=[substr(w_warehouse_name, 1, 20)#37126,sm_type#37117,web_name#7875,30_days#37110L,31_60_days#37111L,61_90_days#37112L,91_120_days#37113L,more_than_120_days#37114L])
   +- HashAggregate(keys=[_groupingexpression#37195, sm_type#37117, web_name#7875], functions=[sum(CASE WHEN ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 30) THEN 1 ELSE 0 END), sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 30) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 60)) THEN 1 ELSE 0 END), sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 60) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 90)) THEN 1 ELSE 0 END), sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 90) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 120)) THEN 1 ELSE 0 END), sum(CASE WHEN ((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 120) THEN 1 ELSE 0 END)], output=[substr(w_warehouse_name, 1, 20)#37126, sm_type#37117, web_name#7875, 30_days#37110L, 31_60_days#37111L, 61_90_days#37112L, 91_120_days#37113L, more_than_120_days#37114L])
      +- Exchange hashpartitioning(_groupingexpression#37195, sm_type#37117, web_name#7875, 200), ENSURE_REQUIREMENTS, [plan_id=119307]
         +- HashAggregate(keys=[_groupingexpression#37195, sm_type#37117, web_name#7875], functions=[partial_sum(CASE WHEN ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 30) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 30) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 60)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 60) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 90)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 90) AND ((ws_ship_date_sk#429 - ws_sold_date_sk#427) <= 120)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN ((ws_ship_date_sk#429 - ws_sold_date_sk#427) > 120) THEN 1 ELSE 0 END)], output=[_groupingexpression#37195, sm_type#37117, web_name#7875, sum#37166L, sum#37167L, sum#37168L, sum#37169L, sum#37170L])
            +- Project [ws_sold_date_sk#427, ws_ship_date_sk#429, sm_type#37117, web_name#7875, substr(w_warehouse_name#21222, 1, 20) AS _groupingexpression#37195]
               +- BroadcastHashJoin [ws_ship_date_sk#429], [d_date_sk#24], Inner, BuildRight, false
                  :- Project [ws_sold_date_sk#427, ws_ship_date_sk#429, w_warehouse_name#21222, sm_type#37117, web_name#7875]
                  :  +- BroadcastHashJoin [ws_web_site_sk#440], [web_site_sk#7871], Inner, BuildRight, false
                  :     :- Project [ws_sold_date_sk#427, ws_ship_date_sk#429, ws_web_site_sk#440, w_warehouse_name#21222, sm_type#37117]
                  :     :  +- BroadcastHashJoin [ws_ship_mode_sk#441], [sm_ship_mode_sk#37115], Inner, BuildRight, false
                  :     :     :- Project [ws_sold_date_sk#427, ws_ship_date_sk#429, ws_web_site_sk#440, ws_ship_mode_sk#441, w_warehouse_name#21222]
                  :     :     :  +- BroadcastHashJoin [ws_warehouse_sk#442], [w_warehouse_sk#21220], Inner, BuildRight, false
                  :     :     :     :- Filter (((isnotnull(ws_warehouse_sk#442) AND isnotnull(ws_ship_mode_sk#441)) AND isnotnull(ws_web_site_sk#440)) AND isnotnull(ws_ship_date_sk#429))
                  :     :     :     :  +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_ship_date_sk#429,ws_web_site_sk#440,ws_ship_mode_sk#441,ws_warehouse_sk#442] Batched: true, DataFilters: [isnotnull(ws_warehouse_sk#442), isnotnull(ws_ship_mode_sk#441), isnotnull(ws_web_site_sk#440), i..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_warehouse_sk), IsNotNull(ws_ship_mode_sk), IsNotNull(ws_web_site_sk), IsNotNull(ws_..., ReadSchema: struct<ws_sold_date_sk:int,ws_ship_date_sk:int,ws_web_site_sk:int,ws_ship_mode_sk:int,ws_warehous...
                  :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=119290]
                  :     :     :        +- Filter isnotnull(w_warehouse_sk#21220)
                  :     :     :           +- FileScan parquet spark_catalog.m.warehouse[w_warehouse_sk#21220,w_warehouse_name#21222] Batched: true, DataFilters: [isnotnull(w_warehouse_sk#21220)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/warehouse], PartitionFilters: [], PushedFilters: [IsNotNull(w_warehouse_sk)], ReadSchema: struct<w_warehouse_sk:int,w_warehouse_name:string>
                  :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=119294]
                  :     :        +- Filter isnotnull(sm_ship_mode_sk#37115)
                  :     :           +- FileScan parquet spark_catalog.m.ship_mode[sm_ship_mode_sk#37115,sm_type#37117] Batched: true, DataFilters: [isnotnull(sm_ship_mode_sk#37115)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/ship_mode], PartitionFilters: [], PushedFilters: [IsNotNull(sm_ship_mode_sk)], ReadSchema: struct<sm_ship_mode_sk:int,sm_type:string>
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=119298]
                  :        +- Filter isnotnull(web_site_sk#7871)
                  :           +- FileScan parquet spark_catalog.m.web_site[web_site_sk#7871,web_name#7875] Batched: true, DataFilters: [isnotnull(web_site_sk#7871)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_site], PartitionFilters: [], PushedFilters: [IsNotNull(web_site_sk)], ReadSchema: struct<web_site_sk:int,web_name:string>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=119302]
                     +- Project [d_date_sk#24]
                        +- Filter (((isnotnull(d_month_seq#27) AND (d_month_seq#27 >= 1178)) AND (d_month_seq#27 <= 1189)) AND isnotnull(d_date_sk#24))
                           +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_month_seq#27] Batched: true, DataFilters: [isnotnull(d_month_seq#27), (d_month_seq#27 >= 1178), (d_month_seq#27 <= 1189), isnotnull(d_date_..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1178), LessThanOrEqual(d_month_seq,1189),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
