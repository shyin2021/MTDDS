AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[ext_price#34062 DESC NULLS LAST,brand_id#34060 ASC NULLS FIRST], output=[brand_id#34060,brand#34061,ext_price#34062])
   +- HashAggregate(keys=[i_brand#1279, i_brand_id#1278], functions=[sum(ss_ext_sales_price#1263)], output=[brand_id#34060, brand#34061, ext_price#34062])
      +- Exchange hashpartitioning(i_brand#1279, i_brand_id#1278, 200), ENSURE_REQUIREMENTS, [plan_id=106179]
         +- HashAggregate(keys=[i_brand#1279, i_brand_id#1278], functions=[partial_sum(ss_ext_sales_price#1263)], output=[i_brand#1279, i_brand_id#1278, sum#34077])
            +- Project [ss_ext_sales_price#1263, i_brand_id#1278, i_brand#1279]
               +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                  :- Project [ss_item_sk#1250, ss_ext_sales_price#1263]
                  :  +- BroadcastHashJoin [d_date_sk#24], [ss_sold_date_sk#1248], Inner, BuildLeft, false
                  :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=106170]
                  :     :  +- Project [d_date_sk#24]
                  :     :     +- Filter ((((isnotnull(d_moy#32) AND isnotnull(d_year#30)) AND (d_moy#32 = 11)) AND (d_year#30 = 2002)) AND isnotnull(d_date_sk#24))
                  :     :        +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_moy#32), isnotnull(d_year#30), (d_moy#32 = 11), (d_year#30 = 2002), isnotnull(d_date..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,11), EqualTo(d_year,2002), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     +- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_item_sk#1250))
                  :        +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_ext_sales_price#1263] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:double>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=106174]
                     +- Project [i_item_sk#1271, i_brand_id#1278, i_brand#1279]
                        +- Filter ((isnotnull(i_manager_id#1291) AND (i_manager_id#1291 = 63)) AND isnotnull(i_item_sk#1271))
                           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_brand_id#1278,i_brand#1279,i_manager_id#1291] Batched: true, DataFilters: [isnotnull(i_manager_id#1291), (i_manager_id#1291 = 63), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,63), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_brand:string,i_manager_id:int>
