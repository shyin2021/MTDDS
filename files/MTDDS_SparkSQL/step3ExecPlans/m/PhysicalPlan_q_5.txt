AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[channel#8024 ASC NULLS FIRST,id#8025 ASC NULLS FIRST], output=[channel#8024,id#8025,sales#7760,returns#7761,profit#7762])
   +- HashAggregate(keys=[channel#8024, id#8025, spark_grouping_id#8023L], functions=[sum(sales#7775), sum(returns#7777), sum(profit#7753)], output=[channel#8024, id#8025, sales#7760, returns#7761, profit#7762])
      +- Exchange hashpartitioning(channel#8024, id#8025, spark_grouping_id#8023L, 200), ENSURE_REQUIREMENTS, [plan_id=8227]
         +- HashAggregate(keys=[channel#8024, id#8025, spark_grouping_id#8023L], functions=[partial_sum(sales#7775), partial_sum(returns#7777), partial_sum(profit#7753)], output=[channel#8024, id#8025, spark_grouping_id#8023L, sum#8053, sum#8054, sum#8055])
            +- Expand [[sales#7775, returns#7777, profit#7753, channel#8021, id#8022, 0], [sales#7775, returns#7777, profit#7753, channel#8021, null, 1], [sales#7775, returns#7777, profit#7753, null, null, 3]], [sales#7775, returns#7777, profit#7753, channel#8024, id#8025, spark_grouping_id#8023L]
               +- Union
                  :- HashAggregate(keys=[s_store_id#53], functions=[sum(sales_price#7765), sum(return_amt#7987), sum(profit#7766), sum(net_loss#7988)], output=[sales#7775, returns#7777, profit#7753, channel#8021, id#8022])
                  :  +- Exchange hashpartitioning(s_store_id#53, 200), ENSURE_REQUIREMENTS, [plan_id=8194]
                  :     +- HashAggregate(keys=[s_store_id#53], functions=[partial_sum(sales_price#7765), partial_sum(return_amt#7987), partial_sum(profit#7766), partial_sum(net_loss#7988)], output=[s_store_id#53, sum#8060, sum#8061, sum#8062, sum#8063])
                  :        +- Project [sales_price#7765, profit#7766, return_amt#7987, net_loss#7988, s_store_id#53]
                  :           +- BroadcastHashJoin [store_sk#7763], [s_store_sk#52], Inner, BuildRight, false
                  :              :- Project [store_sk#7763, sales_price#7765, profit#7766, return_amt#7987, net_loss#7988]
                  :              :  +- BroadcastHashJoin [date_sk#7764], [d_date_sk#24], Inner, BuildRight, false
                  :              :     :- Union
                  :              :     :  :- Project [ss_store_sk#1255 AS store_sk#7763, ss_sold_date_sk#1248 AS date_sk#7764, ss_ext_sales_price#1263 AS sales_price#7765, ss_net_profit#1270 AS profit#7766, 0.0 AS return_amt#7987, 0.0 AS net_loss#7988]
                  :              :     :  :  +- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_store_sk#1255))
                  :              :     :  :     +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_store_sk#1255,ss_ext_sales_price#1263,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_store_sk#1255)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_store_sk:int,ss_ext_sales_price:double,ss_net_profit:double>
                  :              :     :  +- Project [sr_store_sk#11 AS store_sk#7769, sr_returned_date_sk#4 AS date_sk#7770, 0.0 AS sales_price#7989, 0.0 AS profit#7990, sr_return_amt#15 AS return_amt#7773, sr_net_loss#23 AS net_loss#7774]
                  :              :     :     +- Filter (isnotnull(sr_returned_date_sk#4) AND isnotnull(sr_store_sk#11))
                  :              :     :        +- FileScan parquet spark_catalog.m.store_returns[sr_returned_date_sk#4,sr_store_sk#11,sr_return_amt#15,sr_net_loss#23] Batched: true, DataFilters: [isnotnull(sr_returned_date_sk#4), isnotnull(sr_store_sk#11)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_returned_date_sk), IsNotNull(sr_store_sk)], ReadSchema: struct<sr_returned_date_sk:int,sr_store_sk:int,sr_return_amt:double,sr_net_loss:double>
                  :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8185]
                  :              :        +- Project [d_date_sk#24]
                  :              :           +- Filter (((isnotnull(d_date#26) AND (cast(d_date#26 as date) >= 2000-08-19)) AND (cast(d_date#26 as date) <= 2000-09-02)) AND isnotnull(d_date_sk#24))
                  :              :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_date#26] Batched: true, DataFilters: [isnotnull(d_date#26), (cast(d_date#26 as date) >= 2000-08-19), (cast(d_date#26 as date) <= 2000-..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
                  :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=8189]
                  :                 +- Filter isnotnull(s_store_sk#52)
                  :                    +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_store_id#53] Batched: true, DataFilters: [isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_store_id:string>
                  :- HashAggregate(keys=[cp_catalog_page_id#7839], functions=[sum(sales_price#7781), sum(return_amt#7991), sum(profit#7782), sum(net_loss#7992)], output=[sales#7791, returns#7793, profit#7756, channel#8161, id#8162])
                  :  +- Exchange hashpartitioning(cp_catalog_page_id#7839, 200), ENSURE_REQUIREMENTS, [plan_id=8205]
                  :     +- HashAggregate(keys=[cp_catalog_page_id#7839], functions=[partial_sum(sales_price#7781), partial_sum(return_amt#7991), partial_sum(profit#7782), partial_sum(net_loss#7992)], output=[cp_catalog_page_id#7839, sum#8068, sum#8069, sum#8070, sum#8071])
                  :        +- Project [sales_price#7781, profit#7782, return_amt#7991, net_loss#7992, cp_catalog_page_id#7839]
                  :           +- BroadcastHashJoin [page_sk#7779], [cp_catalog_page_sk#7838], Inner, BuildRight, false
                  :              :- Project [page_sk#7779, sales_price#7781, profit#7782, return_amt#7991, net_loss#7992]
                  :              :  +- BroadcastHashJoin [date_sk#7780], [d_date_sk#7897], Inner, BuildRight, false
                  :              :     :- Union
                  :              :     :  :- Project [cs_catalog_page_sk#473 AS page_sk#7779, cs_sold_date_sk#461 AS date_sk#7780, cs_ext_sales_price#484 AS sales_price#7781, cs_net_profit#494 AS profit#7782, 0.0 AS return_amt#7991, 0.0 AS net_loss#7992]
                  :              :     :  :  +- Filter (isnotnull(cs_sold_date_sk#461) AND isnotnull(cs_catalog_page_sk#473))
                  :              :     :  :     +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_catalog_page_sk#473,cs_ext_sales_price#484,cs_net_profit#494] Batched: true, DataFilters: [isnotnull(cs_sold_date_sk#461), isnotnull(cs_catalog_page_sk#473)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_sold_date_sk), IsNotNull(cs_catalog_page_sk)], ReadSchema: struct<cs_sold_date_sk:int,cs_catalog_page_sk:int,cs_ext_sales_price:double,cs_net_profit:double>
                  :              :     :  +- Project [cr_catalog_page_sk#7823 AS page_sk#7785, cr_returned_date_sk#7811 AS date_sk#7786, 0.0 AS sales_price#7993, 0.0 AS profit#7994, cr_return_amount#7829 AS return_amt#7789, cr_net_loss#7837 AS net_loss#7790]
                  :              :     :     +- Filter (isnotnull(cr_returned_date_sk#7811) AND isnotnull(cr_catalog_page_sk#7823))
                  :              :     :        +- FileScan parquet spark_catalog.m.catalog_returns[cr_returned_date_sk#7811,cr_catalog_page_sk#7823,cr_return_amount#7829,cr_net_loss#7837] Batched: true, DataFilters: [isnotnull(cr_returned_date_sk#7811), isnotnull(cr_catalog_page_sk#7823)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_returns], PartitionFilters: [], PushedFilters: [IsNotNull(cr_returned_date_sk), IsNotNull(cr_catalog_page_sk)], ReadSchema: struct<cr_returned_date_sk:int,cr_catalog_page_sk:int,cr_return_amount:double,cr_net_loss:double>
                  :              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8196]
                  :              :        +- Project [d_date_sk#7897]
                  :              :           +- Filter (((isnotnull(d_date#7899) AND (cast(d_date#7899 as date) >= 2000-08-19)) AND (cast(d_date#7899 as date) <= 2000-09-02)) AND isnotnull(d_date_sk#7897))
                  :              :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#7897,d_date#7899] Batched: true, DataFilters: [isnotnull(d_date#7899), (cast(d_date#7899 as date) >= 2000-08-19), (cast(d_date#7899 as date) <=..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
                  :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=8200]
                  :                 +- Filter isnotnull(cp_catalog_page_sk#7838)
                  :                    +- FileScan parquet spark_catalog.m.catalog_page[cp_catalog_page_sk#7838,cp_catalog_page_id#7839] Batched: true, DataFilters: [isnotnull(cp_catalog_page_sk#7838)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_page], PartitionFilters: [], PushedFilters: [IsNotNull(cp_catalog_page_sk)], ReadSchema: struct<cp_catalog_page_sk:int,cp_catalog_page_id:string>
                  +- HashAggregate(keys=[web_site_id#7872], functions=[sum(sales_price#7797), sum(return_amt#7995), sum(profit#7798), sum(net_loss#7996)], output=[sales#7807, returns#7809, profit#7759, channel#8163, id#8164])
                     +- Exchange hashpartitioning(web_site_id#7872, 200), ENSURE_REQUIREMENTS, [plan_id=8221]
                        +- HashAggregate(keys=[web_site_id#7872], functions=[partial_sum(sales_price#7797), partial_sum(return_amt#7995), partial_sum(profit#7798), partial_sum(net_loss#7996)], output=[web_site_id#7872, sum#8076, sum#8077, sum#8078, sum#8079])
                           +- Project [sales_price#7797, profit#7798, return_amt#7995, net_loss#7996, web_site_id#7872]
                              +- BroadcastHashJoin [wsr_web_site_sk#7795], [web_site_sk#7871], Inner, BuildRight, false
                                 :- Project [wsr_web_site_sk#7795, sales_price#7797, profit#7798, return_amt#7995, net_loss#7996]
                                 :  +- BroadcastHashJoin [date_sk#7796], [d_date_sk#7959], Inner, BuildRight, false
                                 :     :- Union
                                 :     :  :- Project [ws_web_site_sk#440 AS wsr_web_site_sk#7795, ws_sold_date_sk#427 AS date_sk#7796, ws_ext_sales_price#450 AS sales_price#7797, ws_net_profit#460 AS profit#7798, 0.0 AS return_amt#7995, 0.0 AS net_loss#7996]
                                 :     :  :  +- Filter (isnotnull(ws_sold_date_sk#427) AND isnotnull(ws_web_site_sk#440))
                                 :     :  :     +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_web_site_sk#440,ws_ext_sales_price#450,ws_net_profit#460] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#427), isnotnull(ws_web_site_sk#440)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_web_site_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_web_site_sk:int,ws_ext_sales_price:double,ws_net_profit:double>
                                 :     :  +- Project [ws_web_site_sk#7938 AS wsr_web_site_sk#7801, wr_returned_date_sk#7847 AS date_sk#7802, 0.0 AS sales_price#7997, 0.0 AS profit#7998, wr_return_amt#7862 AS return_amt#7805, wr_net_loss#7870 AS net_loss#7806]
                                 :     :     +- BroadcastHashJoin [wr_item_sk#7849, wr_order_number#7860], [ws_item_sk#7928, ws_order_number#7942], Inner, BuildLeft, false
                                 :     :        :- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, true] as bigint), 32) | (cast(input[2, int, true] as bigint) & 4294967295))),false), [plan_id=8207]
                                 :     :        :  +- Filter isnotnull(wr_returned_date_sk#7847)
                                 :     :        :     +- FileScan parquet spark_catalog.m.web_returns[wr_returned_date_sk#7847,wr_item_sk#7849,wr_order_number#7860,wr_return_amt#7862,wr_net_loss#7870] Batched: true, DataFilters: [isnotnull(wr_returned_date_sk#7847)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_returned_date_sk)], ReadSchema: struct<wr_returned_date_sk:int,wr_item_sk:int,wr_order_number:int,wr_return_amt:double,wr_net_los...
                                 :     :        +- Filter ((isnotnull(ws_item_sk#7928) AND isnotnull(ws_order_number#7942)) AND isnotnull(ws_web_site_sk#7938))
                                 :     :           +- FileScan parquet spark_catalog.m.web_sales[ws_item_sk#7928,ws_web_site_sk#7938,ws_order_number#7942] Batched: true, DataFilters: [isnotnull(ws_item_sk#7928), isnotnull(ws_order_number#7942), isnotnull(ws_web_site_sk#7938)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_site_sk)], ReadSchema: struct<ws_item_sk:int,ws_web_site_sk:int,ws_order_number:int>
                                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8212]
                                 :        +- Project [d_date_sk#7959]
                                 :           +- Filter (((isnotnull(d_date#7961) AND (cast(d_date#7961 as date) >= 2000-08-19)) AND (cast(d_date#7961 as date) <= 2000-09-02)) AND isnotnull(d_date_sk#7959))
                                 :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#7959,d_date#7961] Batched: true, DataFilters: [isnotnull(d_date#7961), (cast(d_date#7961 as date) >= 2000-08-19), (cast(d_date#7961 as date) <=..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=8216]
                                    +- Filter isnotnull(web_site_sk#7871)
                                       +- FileScan parquet spark_catalog.m.web_site[web_site_sk#7871,web_site_id#7872] Batched: true, DataFilters: [isnotnull(web_site_sk#7871)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_site], PartitionFilters: [], PushedFilters: [IsNotNull(web_site_sk)], ReadSchema: struct<web_site_sk:int,web_site_id:string>
