AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[i_item_id#25579 ASC NULLS FIRST,s_state#25580 ASC NULLS FIRST], output=[i_item_id#25579,s_state#25580,g_state#25567,agg1#25568,agg2#25569,agg3#25570,agg4#25571])
   +- HashAggregate(keys=[i_item_id#25579, s_state#25580, spark_grouping_id#25578L], functions=[avg(ss_quantity#1258), avg(ss_list_price#1260), avg(ss_coupon_amt#1267), avg(ss_sales_price#1261)], output=[i_item_id#25579, s_state#25580, g_state#25567, agg1#25568, agg2#25569, agg3#25570, agg4#25571])
      +- Exchange hashpartitioning(i_item_id#25579, s_state#25580, spark_grouping_id#25578L, 200), ENSURE_REQUIREMENTS, [plan_id=71275]
         +- HashAggregate(keys=[i_item_id#25579, s_state#25580, spark_grouping_id#25578L], functions=[partial_avg(ss_quantity#1258), partial_avg(ss_list_price#1260), partial_avg(ss_coupon_amt#1267), partial_avg(ss_sales_price#1261)], output=[i_item_id#25579, s_state#25580, spark_grouping_id#25578L, sum#25618, count#25619L, sum#25620, count#25621L, sum#25622, count#25623L, sum#25624, count#25625L])
            +- Expand [[ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, i_item_id#1272, s_state#76, 0], [ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, i_item_id#1272, null, 1], [ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, null, null, 3]], [ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, i_item_id#25579, s_state#25580, spark_grouping_id#25578L]
               +- Project [ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, i_item_id#1272, s_state#76]
                  +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                     :- Project [ss_item_sk#1250, ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267, s_state#76]
                     :  +- BroadcastHashJoin [ss_store_sk#1255], [s_store_sk#52], Inner, BuildRight, false
                     :     :- Project [ss_item_sk#1250, ss_store_sk#1255, ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267]
                     :     :  +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
                     :     :     :- Project [ss_sold_date_sk#1248, ss_item_sk#1250, ss_store_sk#1255, ss_quantity#1258, ss_list_price#1260, ss_sales_price#1261, ss_coupon_amt#1267]
                     :     :     :  +- BroadcastHashJoin [ss_cdemo_sk#1252], [cd_demo_sk#8266], Inner, BuildRight, false
                     :     :     :     :- Filter (((isnotnull(ss_cdemo_sk#1252) AND isnotnull(ss_sold_date_sk#1248)) AND isnotnull(ss_store_sk#1255)) AND isnotnull(ss_item_sk#1250))
                     :     :     :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_cdemo_sk#1252,ss_store_sk#1255,ss_quantity#1258,ss_list_price#1260,ss_sales_price#1261,ss_coupon_amt#1267] Batched: true, DataFilters: [isnotnull(ss_cdemo_sk#1252), isnotnull(ss_sold_date_sk#1248), isnotnull(ss_store_sk#1255), isnot..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_cdemo_sk), IsNotNull(ss_sold_date_sk), IsNotNull(ss_store_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_cdemo_sk:int,ss_store_sk:int,ss_quantity:int,ss_list...
                     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=71257]
                     :     :     :        +- Project [cd_demo_sk#8266]
                     :     :     :           +- Filter ((((((isnotnull(cd_gender#8267) AND isnotnull(cd_marital_status#8268)) AND isnotnull(cd_education_status#8269)) AND (cd_gender#8267 = F)) AND (cd_marital_status#8268 = M)) AND (cd_education_status#8269 = Advanced Degree)) AND isnotnull(cd_demo_sk#8266))
                     :     :     :              +- FileScan parquet spark_catalog.m.customer_demographics[cd_demo_sk#8266,cd_gender#8267,cd_marital_status#8268,cd_education_status#8269] Batched: true, DataFilters: [isnotnull(cd_gender#8267), isnotnull(cd_marital_status#8268), isnotnull(cd_education_status#8269..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/customer_demograph..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_gender), IsNotNull(cd_marital_status), IsNotNull(cd_education_status), EqualTo(cd_g..., ReadSchema: struct<cd_demo_sk:int,cd_gender:string,cd_marital_status:string,cd_education_status:string>
                     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=71261]
                     :     :        +- Project [d_date_sk#24]
                     :     :           +- Filter ((isnotnull(d_year#30) AND (d_year#30 = 1998)) AND isnotnull(d_date_sk#24))
                     :     :              +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30] Batched: true, DataFilters: [isnotnull(d_year#30), (d_year#30 = 1998), isnotnull(d_date_sk#24)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,1998), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>
                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=71265]
                     :        +- Filter (s_state#76 IN (TN,AL,SD) AND isnotnull(s_store_sk#52))
                     :           +- FileScan parquet spark_catalog.m.store[s_store_sk#52,s_state#76] Batched: true, DataFilters: [s_state#76 IN (TN,AL,SD), isnotnull(s_store_sk#52)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store], PartitionFilters: [], PushedFilters: [In(s_state, [AL,SD,TN]), IsNotNull(s_store_sk)], ReadSchema: struct<s_store_sk:int,s_state:string>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=71269]
                        +- Filter isnotnull(i_item_sk#1271)
                           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_item_id#1272] Batched: true, DataFilters: [isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string>
