AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[substr(w_warehouse_name, 1, 20)#49482 ASC NULLS FIRST,sm_type#37117 ASC NULLS FIRST,cc_name#20253 ASC NULLS FIRST], output=[substr(w_warehouse_name, 1, 20)#49482,sm_type#37117,cc_name#20253,30_days#49472L,31_60_days#49473L,61_90_days#49474L,91_120_days#49475L,above120_days#49476L])
   +- HashAggregate(keys=[_groupingexpression#49550, sm_type#37117, cc_name#20253], functions=[sum(CASE WHEN ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 30) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 30) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 60)) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 60) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 90)) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 90) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 120)) THEN 1 ELSE 0 END), sum(CASE WHEN ((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 120) THEN 1 ELSE 0 END)], output=[substr(w_warehouse_name, 1, 20)#49482, sm_type#37117, cc_name#20253, 30_days#49472L, 31_60_days#49473L, 61_90_days#49474L, 91_120_days#49475L, above120_days#49476L])
      +- Exchange hashpartitioning(_groupingexpression#49550, sm_type#37117, cc_name#20253, 200), ENSURE_REQUIREMENTS, [plan_id=178847]
         +- HashAggregate(keys=[_groupingexpression#49550, sm_type#37117, cc_name#20253], functions=[partial_sum(CASE WHEN ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 30) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 30) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 60)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 60) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 90)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 90) AND ((cs_ship_date_sk#463 - cs_sold_date_sk#461) <= 120)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN ((cs_ship_date_sk#463 - cs_sold_date_sk#461) > 120) THEN 1 ELSE 0 END)], output=[_groupingexpression#49550, sm_type#37117, cc_name#20253, sum#49521L, sum#49522L, sum#49523L, sum#49524L, sum#49525L])
            +- Project [cs_sold_date_sk#461, cs_ship_date_sk#463, sm_type#37117, cc_name#20253, substr(w_warehouse_name#21222, 1, 20) AS _groupingexpression#49550]
               +- BroadcastHashJoin [cs_ship_date_sk#463], [d_date_sk#24], Inner, BuildRight, false
                  :- Project [cs_sold_date_sk#461, cs_ship_date_sk#463, w_warehouse_name#21222, sm_type#37117, cc_name#20253]
                  :  +- BroadcastHashJoin [cs_call_center_sk#472], [cc_call_center_sk#20247], Inner, BuildRight, false
                  :     :- Project [cs_sold_date_sk#461, cs_ship_date_sk#463, cs_call_center_sk#472, w_warehouse_name#21222, sm_type#37117]
                  :     :  +- BroadcastHashJoin [cs_ship_mode_sk#474], [sm_ship_mode_sk#37115], Inner, BuildRight, false
                  :     :     :- Project [cs_sold_date_sk#461, cs_ship_date_sk#463, cs_call_center_sk#472, cs_ship_mode_sk#474, w_warehouse_name#21222]
                  :     :     :  +- BroadcastHashJoin [cs_warehouse_sk#475], [w_warehouse_sk#21220], Inner, BuildRight, false
                  :     :     :     :- Filter (((isnotnull(cs_warehouse_sk#475) AND isnotnull(cs_ship_mode_sk#474)) AND isnotnull(cs_call_center_sk#472)) AND isnotnull(cs_ship_date_sk#463))
                  :     :     :     :  +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_ship_date_sk#463,cs_call_center_sk#472,cs_ship_mode_sk#474,cs_warehouse_sk#475] Batched: true, DataFilters: [isnotnull(cs_warehouse_sk#475), isnotnull(cs_ship_mode_sk#474), isnotnull(cs_call_center_sk#472)..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_warehouse_sk), IsNotNull(cs_ship_mode_sk), IsNotNull(cs_call_center_sk), IsNotNull(..., ReadSchema: struct<cs_sold_date_sk:int,cs_ship_date_sk:int,cs_call_center_sk:int,cs_ship_mode_sk:int,cs_wareh...
                  :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=178830]
                  :     :     :        +- Filter isnotnull(w_warehouse_sk#21220)
                  :     :     :           +- FileScan parquet spark_catalog.m.warehouse[w_warehouse_sk#21220,w_warehouse_name#21222] Batched: true, DataFilters: [isnotnull(w_warehouse_sk#21220)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/warehouse], PartitionFilters: [], PushedFilters: [IsNotNull(w_warehouse_sk)], ReadSchema: struct<w_warehouse_sk:int,w_warehouse_name:string>
                  :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=178834]
                  :     :        +- Filter isnotnull(sm_ship_mode_sk#37115)
                  :     :           +- FileScan parquet spark_catalog.m.ship_mode[sm_ship_mode_sk#37115,sm_type#37117] Batched: true, DataFilters: [isnotnull(sm_ship_mode_sk#37115)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/ship_mode], PartitionFilters: [], PushedFilters: [IsNotNull(sm_ship_mode_sk)], ReadSchema: struct<sm_ship_mode_sk:int,sm_type:string>
                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=178838]
                  :        +- Filter isnotnull(cc_call_center_sk#20247)
                  :           +- FileScan parquet spark_catalog.m.call_center[cc_call_center_sk#20247,cc_name#20253] Batched: true, DataFilters: [isnotnull(cc_call_center_sk#20247)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/call_center], PartitionFilters: [], PushedFilters: [IsNotNull(cc_call_center_sk)], ReadSchema: struct<cc_call_center_sk:int,cc_name:string>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=178842]
                     +- Project [d_date_sk#24]
                        +- Filter (((isnotnull(d_month_seq#27) AND (d_month_seq#27 >= 1211)) AND (d_month_seq#27 <= 1222)) AND isnotnull(d_date_sk#24))
                           +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_month_seq#27] Batched: true, DataFilters: [isnotnull(d_month_seq#27), (d_month_seq#27 >= 1211), (d_month_seq#27 <= 1222), isnotnull(d_date_..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1211), LessThanOrEqual(d_month_seq,1222),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
