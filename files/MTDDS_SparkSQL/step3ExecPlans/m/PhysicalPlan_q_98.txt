AdaptiveSparkPlan isFinalPlan=false
+- Sort [i_category#1283 ASC NULLS FIRST, i_class#1281 ASC NULLS FIRST, i_item_id#1272 ASC NULLS FIRST, i_item_desc#1275 ASC NULLS FIRST, revenueratio#49422 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(i_category#1283 ASC NULLS FIRST, i_class#1281 ASC NULLS FIRST, i_item_id#1272 ASC NULLS FIRST, i_item_desc#1275 ASC NULLS FIRST, revenueratio#49422 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=178182]
      +- Project [i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, itemrevenue#49421, ((_w0#49427 * 100.0) / _we0#49428) AS revenueratio#49422]
         +- Window [sum(_w0#49427) windowspecdefinition(i_class#1281, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#49428], [i_class#1281]
            +- Sort [i_class#1281 ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(i_class#1281, 200), ENSURE_REQUIREMENTS, [plan_id=178177]
                  +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276], functions=[sum(ss_ext_sales_price#1263)], output=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, itemrevenue#49421, _w0#49427])
                     +- Exchange hashpartitioning(i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, 200), ENSURE_REQUIREMENTS, [plan_id=178174]
                        +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, knownfloatingpointnormalized(normalizenanandzero(i_current_price#1276)) AS i_current_price#1276], functions=[partial_sum(ss_ext_sales_price#1263)], output=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, sum#49458])
                           +- Project [ss_ext_sales_price#1263, i_item_id#1272, i_item_desc#1275, i_current_price#1276, i_class#1281, i_category#1283]
                              +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#24], Inner, BuildRight, false
                                 :- Project [ss_sold_date_sk#1248, ss_ext_sales_price#1263, i_item_id#1272, i_item_desc#1275, i_current_price#1276, i_class#1281, i_category#1283]
                                 :  +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                                 :     :- Filter (isnotnull(ss_item_sk#1250) AND isnotnull(ss_sold_date_sk#1248))
                                 :     :  +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_ext_sales_price#1263] Batched: true, DataFilters: [isnotnull(ss_item_sk#1250), isnotnull(ss_sold_date_sk#1248)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_sold_date_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:double>
                                 :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=178165]
                                 :        +- Filter (i_category#1283 IN (Books,Shoes,Children) AND isnotnull(i_item_sk#1271))
                                 :           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_item_id#1272,i_item_desc#1275,i_current_price#1276,i_class#1281,i_category#1283] Batched: true, DataFilters: [i_category#1283 IN (Books,Shoes,Children), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [In(i_category, [Books,Children,Shoes]), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string,i_current_price:double,i_class:string,i_...
                                 +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=178169]
                                    +- Project [d_date_sk#24]
                                       +- Filter (((isnotnull(d_date#26) AND (cast(d_date#26 as date) >= 2002-01-02)) AND (cast(d_date#26 as date) <= 2002-02-01)) AND isnotnull(d_date_sk#24))
                                          +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_date#26] Batched: true, DataFilters: [isnotnull(d_date#26), (cast(d_date#26 as date) >= 2002-01-02), (cast(d_date#26 as date) <= 2002-..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
