AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[d_year#30 ASC NULLS FIRST,ext_price#33872 DESC NULLS LAST,brand_id#33870 ASC NULLS FIRST], output=[d_year#30,brand_id#33870,brand#33871,ext_price#33872])
   +- HashAggregate(keys=[d_year#30, i_brand#1279, i_brand_id#1278], functions=[sum(ss_ext_sales_price#1263)], output=[d_year#30, brand_id#33870, brand#33871, ext_price#33872])
      +- Exchange hashpartitioning(d_year#30, i_brand#1279, i_brand_id#1278, 200), ENSURE_REQUIREMENTS, [plan_id=103221]
         +- HashAggregate(keys=[d_year#30, i_brand#1279, i_brand_id#1278], functions=[partial_sum(ss_ext_sales_price#1263)], output=[d_year#30, i_brand#1279, i_brand_id#1278, sum#33891])
            +- Project [d_year#30, ss_ext_sales_price#1263, i_brand_id#1278, i_brand#1279]
               +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                  :- Project [d_year#30, ss_item_sk#1250, ss_ext_sales_price#1263]
                  :  +- BroadcastHashJoin [d_date_sk#24], [ss_sold_date_sk#1248], Inner, BuildLeft, false
                  :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=103212]
                  :     :  +- Project [d_date_sk#24, d_year#30]
                  :     :     +- Filter ((((isnotnull(d_moy#32) AND isnotnull(d_year#30)) AND (d_moy#32 = 12)) AND (d_year#30 = 2002)) AND isnotnull(d_date_sk#24))
                  :     :        +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_moy#32), isnotnull(d_year#30), (d_moy#32 = 12), (d_year#30 = 2002), isnotnull(d_date..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,12), EqualTo(d_year,2002), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     +- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_item_sk#1250))
                  :        +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_ext_sales_price#1263] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:double>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=103216]
                     +- Project [i_item_sk#1271, i_brand_id#1278, i_brand#1279]
                        +- Filter ((isnotnull(i_manager_id#1291) AND (i_manager_id#1291 = 1)) AND isnotnull(i_item_sk#1271))
                           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_brand_id#1278,i_brand#1279,i_manager_id#1291] Batched: true, DataFilters: [isnotnull(i_manager_id#1291), (i_manager_id#1291 = 1), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,1), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_brand_id:int,i_brand:string,i_manager_id:int>
