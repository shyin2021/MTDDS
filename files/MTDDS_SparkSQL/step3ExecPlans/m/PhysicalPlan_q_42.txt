AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[sum(ss_ext_sales_price)#31337 DESC NULLS LAST,d_year#30 ASC NULLS FIRST,i_category_id#1282 ASC NULLS FIRST,i_category#1283 ASC NULLS FIRST], output=[d_year#30,i_category_id#1282,i_category#1283,sum(ss_ext_sales_price)#31337])
   +- HashAggregate(keys=[d_year#30, i_category_id#1282, i_category#1283], functions=[sum(ss_ext_sales_price#1263)], output=[d_year#30, i_category_id#1282, i_category#1283, sum(ss_ext_sales_price)#31337])
      +- Exchange hashpartitioning(d_year#30, i_category_id#1282, i_category#1283, 200), ENSURE_REQUIREMENTS, [plan_id=92220]
         +- HashAggregate(keys=[d_year#30, i_category_id#1282, i_category#1283], functions=[partial_sum(ss_ext_sales_price#1263)], output=[d_year#30, i_category_id#1282, i_category#1283, sum#31356])
            +- Project [d_year#30, ss_ext_sales_price#1263, i_category_id#1282, i_category#1283]
               +- BroadcastHashJoin [ss_item_sk#1250], [i_item_sk#1271], Inner, BuildRight, false
                  :- Project [d_year#30, ss_item_sk#1250, ss_ext_sales_price#1263]
                  :  +- BroadcastHashJoin [d_date_sk#24], [ss_sold_date_sk#1248], Inner, BuildLeft, false
                  :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=92211]
                  :     :  +- Project [d_date_sk#24, d_year#30]
                  :     :     +- Filter ((((isnotnull(d_moy#32) AND isnotnull(d_year#30)) AND (d_moy#32 = 12)) AND (d_year#30 = 1999)) AND isnotnull(d_date_sk#24))
                  :     :        +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_moy#32), isnotnull(d_year#30), (d_moy#32 = 12), (d_year#30 = 1999), isnotnull(d_date..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,12), EqualTo(d_year,1999), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
                  :     +- Filter (isnotnull(ss_sold_date_sk#1248) AND isnotnull(ss_item_sk#1250))
                  :        +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_ext_sales_price#1263] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#1248), isnotnull(ss_item_sk#1250)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:double>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=92215]
                     +- Project [i_item_sk#1271, i_category_id#1282, i_category#1283]
                        +- Filter ((isnotnull(i_manager_id#1291) AND (i_manager_id#1291 = 1)) AND isnotnull(i_item_sk#1271))
                           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_category_id#1282,i_category#1283,i_manager_id#1291] Batched: true, DataFilters: [isnotnull(i_manager_id#1291), (i_manager_id#1291 = 1), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,1), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_category_id:int,i_category:string,i_manager_id:int>
