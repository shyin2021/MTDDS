AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[i_category#1283 ASC NULLS FIRST,i_class#1281 ASC NULLS FIRST,i_item_id#1272 ASC NULLS FIRST,i_item_desc#1275 ASC NULLS FIRST,revenueratio#12060 ASC NULLS FIRST], output=[i_item_id#1272,i_item_desc#1275,i_category#1283,i_class#1281,i_current_price#1276,itemrevenue#12059,revenueratio#12060])
   +- Project [i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, itemrevenue#12059, ((_w0#12065 * 100.0) / _we0#12066) AS revenueratio#12060]
      +- Window [sum(_w0#12065) windowspecdefinition(i_class#1281, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#12066], [i_class#1281]
         +- Sort [i_class#1281 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(i_class#1281, 200), ENSURE_REQUIREMENTS, [plan_id=17136]
               +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276], functions=[sum(ws_ext_sales_price#450)], output=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, itemrevenue#12059, _w0#12065])
                  +- Exchange hashpartitioning(i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, 200), ENSURE_REQUIREMENTS, [plan_id=17133]
                     +- HashAggregate(keys=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, knownfloatingpointnormalized(normalizenanandzero(i_current_price#1276)) AS i_current_price#1276], functions=[partial_sum(ws_ext_sales_price#450)], output=[i_item_id#1272, i_item_desc#1275, i_category#1283, i_class#1281, i_current_price#1276, sum#12096])
                        +- Project [ws_ext_sales_price#450, i_item_id#1272, i_item_desc#1275, i_current_price#1276, i_class#1281, i_category#1283]
                           +- BroadcastHashJoin [ws_sold_date_sk#427], [d_date_sk#24], Inner, BuildRight, false
                              :- Project [ws_sold_date_sk#427, ws_ext_sales_price#450, i_item_id#1272, i_item_desc#1275, i_current_price#1276, i_class#1281, i_category#1283]
                              :  +- BroadcastHashJoin [ws_item_sk#430], [i_item_sk#1271], Inner, BuildRight, false
                              :     :- Filter (isnotnull(ws_item_sk#430) AND isnotnull(ws_sold_date_sk#427))
                              :     :  +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_item_sk#430,ws_ext_sales_price#450] Batched: true, DataFilters: [isnotnull(ws_item_sk#430), isnotnull(ws_sold_date_sk#427)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_ext_sales_price:double>
                              :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=17124]
                              :        +- Filter (i_category#1283 IN (Men,Sports,Home) AND isnotnull(i_item_sk#1271))
                              :           +- FileScan parquet spark_catalog.m.item[i_item_sk#1271,i_item_id#1272,i_item_desc#1275,i_current_price#1276,i_class#1281,i_category#1283] Batched: true, DataFilters: [i_category#1283 IN (Men,Sports,Home), isnotnull(i_item_sk#1271)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/item], PartitionFilters: [], PushedFilters: [In(i_category, [Home,Men,Sports]), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_item_id:string,i_item_desc:string,i_current_price:double,i_class:string,i_...
                              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=17128]
                                 +- Project [d_date_sk#24]
                                    +- Filter (((isnotnull(d_date#26) AND (cast(d_date#26 as date) >= 2001-03-04)) AND (cast(d_date#26 as date) <= 2001-04-03)) AND isnotnull(d_date_sk#24))
                                       +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_date#26] Batched: true, DataFilters: [isnotnull(d_date#26), (cast(d_date#26 as date) >= 2001-03-04), (cast(d_date#26 as date) <= 2001-..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_date:string>
