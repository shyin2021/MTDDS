AdaptiveSparkPlan isFinalPlan=false
+- TakeOrderedAndProject(limit=100, orderBy=[channel#33133 ASC NULLS FIRST,return_rank#33131 ASC NULLS FIRST,currency_rank#33132 ASC NULLS FIRST,item#33128 ASC NULLS FIRST], output=[channel#33133,item#33128,return_ratio#33129,return_rank#33131,currency_rank#33132])
   +- HashAggregate(keys=[channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132], functions=[], output=[channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132])
      +- Exchange hashpartitioning(channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132, 200), ENSURE_REQUIREMENTS, [plan_id=100948]
         +- HashAggregate(keys=[channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132], functions=[], output=[channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132])
            +- Union
               :- Project [web AS channel#33133, item#33128, return_ratio#33129, return_rank#33131, currency_rank#33132]
               :  +- Filter ((return_rank#33131 <= 10) OR (currency_rank#33132 <= 10))
               :     +- Window [rank(currency_ratio#33130) windowspecdefinition(currency_ratio#33130 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS currency_rank#33132], [currency_ratio#33130 ASC NULLS FIRST]
               :        +- Sort [currency_ratio#33130 ASC NULLS FIRST], false, 0
               :           +- Window [rank(return_ratio#33129) windowspecdefinition(return_ratio#33129 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS return_rank#33131], [return_ratio#33129 ASC NULLS FIRST]
               :              +- Sort [return_ratio#33129 ASC NULLS FIRST], false, 0
               :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=100897]
               :                    +- HashAggregate(keys=[ws_item_sk#430], functions=[sum(coalesce(wr_return_quantity#7861, 0)), sum(coalesce(ws_quantity#445, 0)), sum(coalesce(wr_return_amt#7862, 0.0)), sum(coalesce(ws_net_paid#456, 0.0))], output=[item#33128, return_ratio#33129, currency_ratio#33130])
               :                       +- Exchange hashpartitioning(ws_item_sk#430, 200), ENSURE_REQUIREMENTS, [plan_id=100894]
               :                          +- HashAggregate(keys=[ws_item_sk#430], functions=[partial_sum(coalesce(wr_return_quantity#7861, 0)), partial_sum(coalesce(ws_quantity#445, 0)), partial_sum(coalesce(wr_return_amt#7862, 0.0)), partial_sum(coalesce(ws_net_paid#456, 0.0))], output=[ws_item_sk#430, sum#33324L, sum#33325L, sum#33628, sum#33629])
               :                             +- Project [ws_item_sk#430, ws_quantity#445, ws_net_paid#456, wr_return_quantity#7861, wr_return_amt#7862]
               :                                +- BroadcastHashJoin [ws_sold_date_sk#427], [d_date_sk#24], Inner, BuildRight, false
               :                                   :- Project [ws_sold_date_sk#427, ws_item_sk#430, ws_quantity#445, ws_net_paid#456, wr_return_quantity#7861, wr_return_amt#7862]
               :                                   :  +- BroadcastHashJoin [ws_order_number#444, ws_item_sk#430], [wr_order_number#7860, wr_item_sk#7849], Inner, BuildRight, false
               :                                   :     :- Project [ws_sold_date_sk#427, ws_item_sk#430, ws_order_number#444, ws_quantity#445, ws_net_paid#456]
               :                                   :     :  +- Filter ((((((((isnotnull(ws_net_profit#460) AND isnotnull(ws_net_paid#456)) AND isnotnull(ws_quantity#445)) AND (ws_net_profit#460 > 1.0)) AND (ws_net_paid#456 > 0.0)) AND (ws_quantity#445 > 0)) AND isnotnull(ws_order_number#444)) AND isnotnull(ws_item_sk#430)) AND isnotnull(ws_sold_date_sk#427))
               :                                   :     :     +- FileScan parquet spark_catalog.m.web_sales[ws_sold_date_sk#427,ws_item_sk#430,ws_order_number#444,ws_quantity#445,ws_net_paid#456,ws_net_profit#460] Batched: true, DataFilters: [isnotnull(ws_net_profit#460), isnotnull(ws_net_paid#456), isnotnull(ws_quantity#445), (ws_net_pr..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_net_profit), IsNotNull(ws_net_paid), IsNotNull(ws_quantity), GreaterThan(ws_net_pro..., ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_order_number:int,ws_quantity:int,ws_net_paid:double,...
               :                                   :     +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, false] as bigint), 32) | (cast(input[0, int, false] as bigint) & 4294967295))),false), [plan_id=100885]
               :                                   :        +- Filter (((isnotnull(wr_return_amt#7862) AND (wr_return_amt#7862 > 10000.0)) AND isnotnull(wr_order_number#7860)) AND isnotnull(wr_item_sk#7849))
               :                                   :           +- FileScan parquet spark_catalog.m.web_returns[wr_item_sk#7849,wr_order_number#7860,wr_return_quantity#7861,wr_return_amt#7862] Batched: true, DataFilters: [isnotnull(wr_return_amt#7862), (wr_return_amt#7862 > 10000.0), isnotnull(wr_order_number#7860), ..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/web_returns], PartitionFilters: [], PushedFilters: [IsNotNull(wr_return_amt), GreaterThan(wr_return_amt,10000.0), IsNotNull(wr_order_number), IsNotN..., ReadSchema: struct<wr_item_sk:int,wr_order_number:int,wr_return_quantity:int,wr_return_amt:double>
               :                                   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=100889]
               :                                      +- Project [d_date_sk#24]
               :                                         +- Filter ((((isnotnull(d_year#30) AND isnotnull(d_moy#32)) AND (d_year#30 = 2002)) AND (d_moy#32 = 12)) AND isnotnull(d_date_sk#24))
               :                                            +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#24,d_year#30,d_moy#32] Batched: true, DataFilters: [isnotnull(d_year#30), isnotnull(d_moy#32), (d_year#30 = 2002), (d_moy#32 = 12), isnotnull(d_date..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2002), EqualTo(d_moy,12), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               :- Project [catalog AS channel#33139, item#33134, return_ratio#33135, return_rank#33137, currency_rank#33138]
               :  +- Filter ((return_rank#33137 <= 10) OR (currency_rank#33138 <= 10))
               :     +- Window [rank(currency_ratio#33136) windowspecdefinition(currency_ratio#33136 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS currency_rank#33138], [currency_ratio#33136 ASC NULLS FIRST]
               :        +- Sort [currency_ratio#33136 ASC NULLS FIRST], false, 0
               :           +- Window [rank(return_ratio#33135) windowspecdefinition(return_ratio#33135 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS return_rank#33137], [return_ratio#33135 ASC NULLS FIRST]
               :              +- Sort [return_ratio#33135 ASC NULLS FIRST], false, 0
               :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=100917]
               :                    +- HashAggregate(keys=[cs_item_sk#476], functions=[sum(coalesce(cr_return_quantity#7828, 0)), sum(coalesce(cs_quantity#479, 0)), sum(coalesce(cr_return_amount#7829, 0.0)), sum(coalesce(cs_net_paid#490, 0.0))], output=[item#33134, return_ratio#33135, currency_ratio#33136])
               :                       +- Exchange hashpartitioning(cs_item_sk#476, 200), ENSURE_REQUIREMENTS, [plan_id=100914]
               :                          +- HashAggregate(keys=[cs_item_sk#476], functions=[partial_sum(coalesce(cr_return_quantity#7828, 0)), partial_sum(coalesce(cs_quantity#479, 0)), partial_sum(coalesce(cr_return_amount#7829, 0.0)), partial_sum(coalesce(cs_net_paid#490, 0.0))], output=[cs_item_sk#476, sum#33332L, sum#33333L, sum#33632, sum#33633])
               :                             +- Project [cs_item_sk#476, cs_quantity#479, cs_net_paid#490, cr_return_quantity#7828, cr_return_amount#7829]
               :                                +- BroadcastHashJoin [cs_sold_date_sk#461], [d_date_sk#33158], Inner, BuildRight, false
               :                                   :- Project [cs_sold_date_sk#461, cs_item_sk#476, cs_quantity#479, cs_net_paid#490, cr_return_quantity#7828, cr_return_amount#7829]
               :                                   :  +- BroadcastHashJoin [cs_order_number#478, cs_item_sk#476], [cr_order_number#7827, cr_item_sk#7813], Inner, BuildRight, false
               :                                   :     :- Project [cs_sold_date_sk#461, cs_item_sk#476, cs_order_number#478, cs_quantity#479, cs_net_paid#490]
               :                                   :     :  +- Filter ((((((((isnotnull(cs_net_profit#494) AND isnotnull(cs_net_paid#490)) AND isnotnull(cs_quantity#479)) AND (cs_net_profit#494 > 1.0)) AND (cs_net_paid#490 > 0.0)) AND (cs_quantity#479 > 0)) AND isnotnull(cs_order_number#478)) AND isnotnull(cs_item_sk#476)) AND isnotnull(cs_sold_date_sk#461))
               :                                   :     :     +- FileScan parquet spark_catalog.m.catalog_sales[cs_sold_date_sk#461,cs_item_sk#476,cs_order_number#478,cs_quantity#479,cs_net_paid#490,cs_net_profit#494] Batched: true, DataFilters: [isnotnull(cs_net_profit#494), isnotnull(cs_net_paid#490), isnotnull(cs_quantity#479), (cs_net_pr..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_sales], PartitionFilters: [], PushedFilters: [IsNotNull(cs_net_profit), IsNotNull(cs_net_paid), IsNotNull(cs_quantity), GreaterThan(cs_net_pro..., ReadSchema: struct<cs_sold_date_sk:int,cs_item_sk:int,cs_order_number:int,cs_quantity:int,cs_net_paid:double,...
               :                                   :     +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, false] as bigint), 32) | (cast(input[0, int, false] as bigint) & 4294967295))),false), [plan_id=100905]
               :                                   :        +- Filter (((isnotnull(cr_return_amount#7829) AND (cr_return_amount#7829 > 10000.0)) AND isnotnull(cr_order_number#7827)) AND isnotnull(cr_item_sk#7813))
               :                                   :           +- FileScan parquet spark_catalog.m.catalog_returns[cr_item_sk#7813,cr_order_number#7827,cr_return_quantity#7828,cr_return_amount#7829] Batched: true, DataFilters: [isnotnull(cr_return_amount#7829), (cr_return_amount#7829 > 10000.0), isnotnull(cr_order_number#7..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/catalog_returns], PartitionFilters: [], PushedFilters: [IsNotNull(cr_return_amount), GreaterThan(cr_return_amount,10000.0), IsNotNull(cr_order_number), ..., ReadSchema: struct<cr_item_sk:int,cr_order_number:int,cr_return_quantity:int,cr_return_amount:double>
               :                                   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=100909]
               :                                      +- Project [d_date_sk#33158]
               :                                         +- Filter ((((isnotnull(d_year#33164) AND isnotnull(d_moy#33166)) AND (d_year#33164 = 2002)) AND (d_moy#33166 = 12)) AND isnotnull(d_date_sk#33158))
               :                                            +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#33158,d_year#33164,d_moy#33166] Batched: true, DataFilters: [isnotnull(d_year#33164), isnotnull(d_moy#33166), (d_year#33164 = 2002), (d_moy#33166 = 12), isno..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2002), EqualTo(d_moy,12), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
               +- Project [store AS channel#33145, item#33140, return_ratio#33141, return_rank#33143, currency_rank#33144]
                  +- Filter ((return_rank#33143 <= 10) OR (currency_rank#33144 <= 10))
                     +- Window [rank(currency_ratio#33142) windowspecdefinition(currency_ratio#33142 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS currency_rank#33144], [currency_ratio#33142 ASC NULLS FIRST]
                        +- Sort [currency_ratio#33142 ASC NULLS FIRST], false, 0
                           +- Window [rank(return_ratio#33141) windowspecdefinition(return_ratio#33141 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS return_rank#33143], [return_ratio#33141 ASC NULLS FIRST]
                              +- Sort [return_ratio#33141 ASC NULLS FIRST], false, 0
                                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=100937]
                                    +- HashAggregate(keys=[ss_item_sk#1250], functions=[sum(coalesce(sr_return_quantity#14, 0)), sum(coalesce(ss_quantity#1258, 0)), sum(coalesce(sr_return_amt#15, 0.0)), sum(coalesce(ss_net_paid#1268, 0.0))], output=[item#33140, return_ratio#33141, currency_ratio#33142])
                                       +- Exchange hashpartitioning(ss_item_sk#1250, 200), ENSURE_REQUIREMENTS, [plan_id=100934]
                                          +- HashAggregate(keys=[ss_item_sk#1250], functions=[partial_sum(coalesce(sr_return_quantity#14, 0)), partial_sum(coalesce(ss_quantity#1258, 0)), partial_sum(coalesce(sr_return_amt#15, 0.0)), partial_sum(coalesce(ss_net_paid#1268, 0.0))], output=[ss_item_sk#1250, sum#33340L, sum#33341L, sum#33636, sum#33637])
                                             +- Project [ss_item_sk#1250, ss_quantity#1258, ss_net_paid#1268, sr_return_quantity#14, sr_return_amt#15]
                                                +- BroadcastHashJoin [ss_sold_date_sk#1248], [d_date_sk#33186], Inner, BuildRight, false
                                                   :- Project [ss_sold_date_sk#1248, ss_item_sk#1250, ss_quantity#1258, ss_net_paid#1268, sr_return_quantity#14, sr_return_amt#15]
                                                   :  +- BroadcastHashJoin [ss_ticket_number#1257, ss_item_sk#1250], [sr_ticket_number#13, sr_item_sk#6], Inner, BuildRight, false
                                                   :     :- Project [ss_sold_date_sk#1248, ss_item_sk#1250, ss_ticket_number#1257, ss_quantity#1258, ss_net_paid#1268]
                                                   :     :  +- Filter ((((((((isnotnull(ss_net_profit#1270) AND isnotnull(ss_net_paid#1268)) AND isnotnull(ss_quantity#1258)) AND (ss_net_profit#1270 > 1.0)) AND (ss_net_paid#1268 > 0.0)) AND (ss_quantity#1258 > 0)) AND isnotnull(ss_ticket_number#1257)) AND isnotnull(ss_item_sk#1250)) AND isnotnull(ss_sold_date_sk#1248))
                                                   :     :     +- FileScan parquet spark_catalog.m.store_sales[ss_sold_date_sk#1248,ss_item_sk#1250,ss_ticket_number#1257,ss_quantity#1258,ss_net_paid#1268,ss_net_profit#1270] Batched: true, DataFilters: [isnotnull(ss_net_profit#1270), isnotnull(ss_net_paid#1268), isnotnull(ss_quantity#1258), (ss_net..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ss_net_profit), IsNotNull(ss_net_paid), IsNotNull(ss_quantity), GreaterThan(ss_net_pro..., ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ticket_number:int,ss_quantity:int,ss_net_paid:double...
                                                   :     +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft(cast(input[1, int, false] as bigint), 32) | (cast(input[0, int, false] as bigint) & 4294967295))),false), [plan_id=100925]
                                                   :        +- Filter (((isnotnull(sr_return_amt#15) AND (sr_return_amt#15 > 10000.0)) AND isnotnull(sr_ticket_number#13)) AND isnotnull(sr_item_sk#6))
                                                   :           +- FileScan parquet spark_catalog.m.store_returns[sr_item_sk#6,sr_ticket_number#13,sr_return_quantity#14,sr_return_amt#15] Batched: true, DataFilters: [isnotnull(sr_return_amt#15), (sr_return_amt#15 > 10000.0), isnotnull(sr_ticket_number#13), isnot..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/store_returns], PartitionFilters: [], PushedFilters: [IsNotNull(sr_return_amt), GreaterThan(sr_return_amt,10000.0), IsNotNull(sr_ticket_number), IsNot..., ReadSchema: struct<sr_item_sk:int,sr_ticket_number:int,sr_return_quantity:int,sr_return_amt:double>
                                                   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=100929]
                                                      +- Project [d_date_sk#33186]
                                                         +- Filter ((((isnotnull(d_year#33192) AND isnotnull(d_moy#33194)) AND (d_year#33192 = 2002)) AND (d_moy#33194 = 12)) AND isnotnull(d_date_sk#33186))
                                                            +- FileScan parquet spark_catalog.m.date_dim[d_date_sk#33186,d_year#33192,d_moy#33194] Batched: true, DataFilters: [isnotnull(d_year#33192), isnotnull(d_moy#33194), (d_year#33192 = 2002), (d_moy#33194 = 12), isno..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://gros-121:9000/usr/spark/spark-warehouse/m.db/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2002), EqualTo(d_moy,12), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
