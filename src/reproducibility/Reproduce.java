package reproducibility;

import java.io.IOException;
import java.sql.SQLException;

public class Reproduce {

	public static void main(String args[]) throws ClassNotFoundException, SQLException, IOException {
		
		// Generate 20 tenants to reproduce Figures 2-4 in the paper
		// Input: number of tenants to generate
		// Output: a csv file containing the tenants' information
		core.TenantsGenerator.generateTenants(20, "files\\benchmarkInputFiles\\TnInterArv.csv", "files\\20tenants.csv");
		System.out.println("[REPRODUCE] 20 tenants are generated.");
		
		// Generate queries for the 20 tenants
		// Input: a csv file containing the tenants' information
		// Output: a csv file containing all queries in the folder 20tenantsQueries
		core.TenantQueriesGenerator.generateAllQueries_OneFile("files\\20tenants.csv", "files\\20tenantsQueries");
		System.out.println("[REPRODUCE] Queries for the 20 tenants are generated.");
		
		// the file 20tenantsQueries.csv was then imported into the Excel file QueryArrivals.xlsx for plotting the figures (see the folder excelFilesforFigures).
		
		/*** The benchmarking steps used by the experiments in Section 4 are below. ***/
		/*** Step 1 - Data and queries generation ***/
		System.out.println("\n====== Step 1 - Data and queries generation ======");
				
		/* We generated 3 data sets (scale factor 1, 2 and 3) and the 99 queries for each scale factor by using the TPC-DS tool kit. */
		System.out.println("### We generated 3 data sets (scale factor 1, 2 and 3) and the 99 queries for each scale factor by using the TPC-DS tool kit.");
		
		// With the ErrorCorrectionForPG.fixSyntaxErrors() method, we fix several syntax errors with respect to PostgreSQL
		// Input: a file in the folder step1Queries_TPCDS with the queries generated by TPC-DS tool kit (using the dialect netezza)
		// Output: a file in the same folder called query_0_corrected.sql
		example.ErrorCorrectionForPG.fixSyntaxErrors("files\\step1Queries_TPCDS\\l\\query_0.sql", "files\\step1Queries_TPCDS\\l\\query_0_corrected.sql");
		example.ErrorCorrectionForPG.fixSyntaxErrors("files\\step1Queries_TPCDS\\m\\query_0.sql", "files\\step1Queries_TPCDS\\m\\query_0_corrected.sql");
		example.ErrorCorrectionForPG.fixSyntaxErrors("files\\step1Queries_TPCDS\\s\\query_0.sql", "files\\step1Queries_TPCDS\\s\\query_0_corrected.sql");
		System.out.println("[REPRODUCE] Syntax errors are fixed.");
		
		// With the addExplain_( ) method in the example.AddExplain class, we then add EXPLAIN ANALYZE to each query, in order that the execution plans will be printed during the Power Test. 
		// Input: a file in the folder step1Queries_TPCDS with the queries generated by TPC-DS tool kit
		// Output: a file in the folder step1QueriesWithExplain with the queries preceded by EXPLAIN ANALYZE
		example.AddExplain.addExplain_("files\\step1Queries_TPCDS\\l\\query_0_corrected.sql", "files\\step1QueriesWithExplain\\l", "query_0_l", "/root/home/postgres/mtdds/files/step2ExecPlans/l");
		example.AddExplain.addExplain_("files\\step1Queries_TPCDS\\m\\query_0_corrected.sql", "files\\step1QueriesWithExplain\\m", "query_0_m", "/root/home/postgres/mtdds/files/step2ExecPlans/m");
		example.AddExplain.addExplain_("files\\step1Queries_TPCDS\\s\\query_0_corrected.sql", "files\\step1QueriesWithExplain\\s", "query_0_s", "/root/home/postgres/mtdds/files/step2ExecPlans/s");		
		System.out.println("[REPRODUCE] EXPLAIN ANALYZE is added.");
		
		// With the EnableNLforExists.addCmd() method, we add the command "set enable_nestloop = on" to several queries
		// Input: a file in the folder step1QueriesWithExplain (e.g., query_0_s.sql)
		// Output: a file in the same folder (query_0_s_nl_enabled.sql)
		example.EnableNLforExists.addCmd("files\\step1QueriesWithExplain\\l\\query_0_l.sql", "files\\step1QueriesWithExplain\\l\\query_0_l_nl_enabled.sql");
		example.EnableNLforExists.addCmd("files\\step1QueriesWithExplain\\m\\query_0_m.sql", "files\\step1QueriesWithExplain\\m\\query_0_m_nl_enabled.sql");
		example.EnableNLforExists.addCmd("files\\step1QueriesWithExplain\\s\\query_0_s.sql", "files\\step1QueriesWithExplain\\s\\query_0_s_nl_enabled.sql");
		
		// With RewriteQueries_1_6_10_35.rewrite(), we rewrite the six queries (q1, q6, q10, q30, q35, q81) so that Postgres-XL could execute them faster, 
		// knowing that this modification has no impact on the conclusion, since we do this for both SUTs (Systems Under Test)
		// Input: a file in the folder step1QueriesWithExplain (e.g., query_0_s_nl_enabled.sql)
		// Output: a file in the same folder (query_0_s_pgxl.sql)
		example.RewriteQueries_1_6_10_30_35_81.rewrite("files\\step1QueriesWithExplain\\l", "query_0_l_nl_enabled.sql", "query_0_l_pgxl.sql");
		example.RewriteQueries_1_6_10_30_35_81.rewrite("files\\step1QueriesWithExplain\\m", "query_0_m_nl_enabled.sql", "query_0_m_pgxl.sql");
		example.RewriteQueries_1_6_10_30_35_81.rewrite("files\\step1QueriesWithExplain\\s", "query_0_s_nl_enabled.sql", "query_0_s_pgxl.sql");
		
		/*** Step2 - Performance SLO generation ***/
		System.out.println("\n====== Step 2 - Performance SLO generation ======");
		
		/* We loaded the three databases by using the scripts in the folder step2Scripts_PostgresXL. */
		System.out.println("### We loaded the three databases by using the scripts in the folder step2Scripts_PostgresXL.");
		
		// In our experiments, during the Power Tests, we register the execution plan and execution time of each query for each scale factor in separated files.
		System.out.println("### We registered the execution plan and execution time of each query for each scale factor in separated files (folder step3ExecPlans).");
		// These files (in the folder step2ExecPlans) are then read by the ExecTimeExtractor to extract the execution times.
		// Input: a folder with files containing the execution plan and execution time of each query
		// Output: a single csv file with execution time of each query 
		//example.ExecTimeExtractor.extractExecTime("files\\step2ExecPlans\\l", "files\\step2ExecTimes\\SUT1\\ExecTime_100.csv");
		//example.ExecTimeExtractor.extractExecTime("files\\step2ExecPlans\\m", "files\\step2ExecTimes\\SUT1\\ExecTime_10.csv");
		//example.ExecTimeExtractor.extractExecTime("files\\step2ExecPlans\\s", "files\\step2ExecTimes\\SUT1\\ExecTime_1.csv");
		example.ExecTimeExtractor2.extractExecTime("files\\step2ExecPlans\\l", "files\\step2ExecTimes\\SUT1_2\\ExecTime_100.csv");
		example.ExecTimeExtractor2.extractExecTime("files\\step2ExecPlans\\m", "files\\step2ExecTimes\\SUT1_2\\ExecTime_10.csv");
		example.ExecTimeExtractor2.extractExecTime("files\\step2ExecPlans\\s", "files\\step2ExecTimes\\SUT1_2\\ExecTime_1.csv");
		System.out.println("[REPRODUCE] Execution times extracted.");
		
		// Input 1: directory(execTimeDir) with execTime_*.csv (queryId, execTime) for the three scale factors, for example, execTime_10.csv for scale factor 10 
		// Input 2: query_types.csv (queryId, type)
		// Output: perfSLOs.csv (queryId, scaleFactor, expectedQCT, perfSLO_premium, perfSLO_standard, perfSLO_basic)
		core.PerfSLOGenerator.generatePerfSLO("files\\step2ExecTimes\\SUT1_2", "files\\benchmarkInputFiles\\query_types.csv", "files\\benchmarkInputFiles\\PriorityTRT.csv", "files\\step2PerfSLOs\\perfSLOs_SUT1.csv");
		core.PerfSLOGenerator.generatePerfSLO("files\\step2ExecTimes\\SUT1_2", "files\\benchmarkInputFiles\\query_types.csv", "files\\benchmarkInputFiles\\PriorityTRT.csv", "files\\step2PerfSLOs\\perfSLOs_SUT2.csv");
		core.PerfSLOGenerator.generatePerfSLO("files\\step2ExecTimes\\SUT3_4", "files\\benchmarkInputFiles\\query_types.csv", "files\\benchmarkInputFiles\\PriorityTRT.csv", "files\\step2PerfSLOs\\perfSLOs_SUT3.csv");
		core.PerfSLOGenerator.generatePerfSLO("files\\step2ExecTimes\\SUT3_4", "files\\benchmarkInputFiles\\query_types.csv", "files\\benchmarkInputFiles\\PriorityTRT.csv", "files\\step2PerfSLOs\\perfSLOs_SUT4.csv");
		System.out.println("[REPRODUCE] Performance SLOs generated.");
		
		/*** Step 3 - Multi-tenant query workload generation ***/
		System.out.println("\n====== Step 3 - Multi-tenant query workload generation ======");
		
		// Generate 6 tenants
		// Input: number of tenants to generate
		// Output: a csv file containing the tenants' information
		core.TenantsGenerator.generateTenants(6, "files\\benchmarkInputFiles\\TnInterArv.csv", "files\\6tenants.csv");
		System.out.println("[REPRODUCE] 6 tenants are generated.");
				
		// Generate queries for the 20 tenants
		// Input: a csv file containing the tenants' information
		// Output: a folder with csv files each containing the queries of a tenant
		core.TenantQueriesGenerator.generateAllQueries("files\\6tenants.csv", "files\\6tenantsQueries");
		System.out.println("[REPRODUCE] Queries for the 6 tenants are generated.");
		
		// Split the query_0.sql file for each DBSize into separate sql files (one file for each query), such that in Step 6, each query could be launched at a precise moment
		// Input: a single file in the folder step1QueriesWithExplain (e.g., query_0_s_pgxl.sql)
		// Output: 99 files each containing a single query
		example.splitQueries.splitQ("files\\step1QueriesWithExplain\\l\\query_0_l_pgxl.sql", "files\\step3IndividualQueries\\l");
		example.splitQueries.splitQ("files\\step1QueriesWithExplain\\m\\query_0_m_pgxl.sql", "files\\step3IndividualQueries\\m");
		example.splitQueries.splitQ("files\\step1QueriesWithExplain\\s\\query_0_s_pgxl.sql", "files\\step3IndividualQueries\\s");
		System.out.println("[REPRODUCE] Individual queries are extracted into separate files.");
		
		/*** Step 4 - Multi-tenant query workload tests ***/
		System.out.println("\n====== Step 4 - Multi-tenant query workload tests ======");
		
		// Use templates to generate scripts for creating the 6 tenants, their databases and loading the data
		// Input: a folder (step6SourceScript) containing the initial CREATE TABLE commands provided by the TPC-DS tool kit 
		// Output: a folder with necessary scripts to create tenants, databases and to load the data (step6Scripts)
		// The scripts tpcds_ri.sql, alter_table_rep.sql and create_index_catalog.sql are included in the folder step6Scripts and they can be used by all tenants.
		example.MTTestsScriptWriter.generateScripts(1, 10, 1, 4, 8, "files\\6tenants.csv", "files\\step4SourceScript", "files\\step4Scripts_PostgresXL");
		System.out.println("[REPRODUCE] Multi-tenant databases creation and data load scripts are generated.");
		
		// In our experiments, during theMulti-tenant query workload Tests, we run the driver programs which send queries to the SUTs.
		System.out.println("### We ran the driver program (in the folder step4Drivers_PostgresXL) on each SUT.");
		
		/*** Step 5 - PO_Metric 1 and PO_Metric 2 measurement ***/
		System.out.println("\n====== Step 5 - PO_Metric 1 and PO_Metric 2 measurement ======");
		System.out.println("### In the benchmark we can choose to run Step 5 or Step 6, or both. In our experiment, we ran only Step 6.");
		
		/*** Step 6 - PO_Metric 1 Bis and PO_Metric 2 Bis measurement ***/
		System.out.println("\n====== Step 6 - PO_Metric 1 Bis and PO_Metric 2 Bis measurement ======");
		System.out.println("### We ran the driver program for each Arrival Rate Factor (ARF) and copied the execution traces into the folder step6ExecTraces.");
		
		// In order to compute the metrics' values easily, we use a SQLite database (mtdds.db) to store the parameter values, execution traces and intermediate results.
		
		// Initialize the SQLite database (mtdds.db).
		System.out.println("[REPRODUCE] Creating and initializing a SQLite database mtdds.db to compute the metric's values...");
		tools.DBInitialization.initDB();
		System.out.println("[REPRODUCE] SQLite database mtdds.db initialized.");
		
		// Transform the execution traces from Format1 to Format2, and load them into mtdds.db
		// Input: Execution traces with Format1 (TenantName, QueryNumber, ThreadName, Event, TimeStamp)
		// Output: Execution traces with Format2 (SUTNumber, clusterSize, arrivalRateFactor, TenantName, QueryNumber, ThreadName, LaunchTime, StartTime, FinishTime);
		System.out.println("[REPRODUCE] Transform the execution traces from Format1 to Format2...");
		//tools.ExecutionTraceTransformer.transformAllTraces(1, "files\\step6ExecTraces\\SUT1", 20, "files\\step6FormatedTraces\\SUT1", 11, 1);
		//tools.ExecutionTraceTransformer.transformAllTraces(2, "files\\step6ExecTraces\\SUT2", 20, "files\\step6FormatedTraces\\SUT2", 11, 1);
		//tools.ExecutionTraceTransformer.transformAllTraces(3, "files\\step6ExecTraces\\SUT3", 20, "files\\step6FormatedTraces\\SUT3", 11, 10);
		//tools.ExecutionTraceTransformer.transformAllTracesSUT4(4, "files\\step6ExecTraces\\SUT4", 20, "files\\step6FormatedTraces\\SUT4", 11, 10);
		System.out.println("[REPRODUCE] Execution Traces transformed.");
		
		/*** Step 7 - Final scores computation ***/
		System.out.println("\n====== Step 7 - Final scores computation ======");
		
		// Load the benchmark input files with the prices of the resources, scales factors of various DBSizes and tolerance rate thresholds of tenants with different priorities
		// Input: files in the Folder benchmarkInputFiles
		// Output: updated SQLite database (mtdds.db)
		tools.InputFilesLoader.loadRSPrices("files\\benchmarkInputFiles\\RSPrices.csv");
		tools.InputFilesLoader.loadDBSizesSF("files\\benchmarkInputFiles\\DBSizesSF.csv");
		tools.InputFilesLoader.loadPriorityTRT("files\\benchmarkInputFiles\\PriorityTRT.csv");
		System.out.println("[REPRODUCE] Benchmark input files loaded into mtdds.db.");
		
		// Load the tenants file generated in Step 3
		tools.TenantsLoader.loadTenants("files\\6tenants.csv");
		System.out.println("[REPRODUCE] Generated tenants loaded into mtdds.db.");
		
		// ****** compare SUT1 and SUT2
		// Load the generic performance SLOs generated in Step 2 and compute the performance SLOs for each tenant 
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT1.csv", "SUT1");
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT2.csv", "SUT2");
		tools.PerfSLOsPerTenant.ComputePerfSLOs("SUT1", "SUT2", "files\\benchmarkInputFiles\\PriorityTRT.csv");
		tools.PerfSLOsPerTenant.computePerfSLOs_per_tenant("files\\6tenants.csv");
		System.out.println("[REPRODUCE] Performance SLOs are computed for all tenants.");
		
		// Compute intermediate metrics for SUT1 and SUT2 using the pricing model RCB
		core.PricingModelRCB.computeIntermediateMetrics(1, 11, 18, 1, 10, false);
		core.PricingModelRCB.computeIntermediateMetrics(2, 11, 18, 1, 10, false);
		System.out.println("[REPRODUCE] Intermediate metrics computed for the pricing model RCB.");
		
		
		// Compute intermediate metrics for SUT1 and SUT2 using the pricing model QLSA
		core.PricingModelQLSA.computeIntermediateMetrics(1, 11, 18, 1, 20, false);
		core.PricingModelQLSA.computeIntermediateMetrics(2, 11, 18, 1, 20, false);
		System.out.println("[REPRODUCE] Intermediate metrics computed for the pricing model QLSA.");
		
		// Compute the final scores of SUT1 and SUT2 using the pricing model RCB under the clusterSize 11 with SSR_MIN = 0.7
		System.out.println("[REPRODUCE] Computing final scores for the pricing model RCB...");
		System.out.println("== SUT1: HARF related results ==");
		core.Metrics.showAllforHARF(1, "RCB", 11, 0.7);
		System.out.println("== SUT1: OARF related results ==");
		core.Metrics.showAllforOARF(1, "RCB", 11, 0.7);
		System.out.println("== SUT2: HARF related results ==");
		core.Metrics.showAllforHARF(2, "RCB", 11, 0.7);
		System.out.println("== SUT2: OARF related results ==");
		core.Metrics.showAllforOARF(2, "RCB", 11, 0.7);
		System.out.println("[REPRODUCE] Final scores computed for the pricing model RCB.");
		
		// Compute the final scores of SUT1 and SUT2 using the pricing model QLSA under the clusterSize 11 with SSR_MIN = 0.7
		System.out.println("[REPRODUCE] Computing final scores for the pricing model QLSA...");
		System.out.println("== SUT1: HARF related resqults ==");
		core.Metrics.showAllforHARF(1, "QLSA", 11, 0.7);
		System.out.println("== SUT1: OARF related results ==");
		core.Metrics.showAllforOARF(1, "QLSA", 11, 0.7);
		System.out.println("== SUT2: HARF related results ==");
		core.Metrics.showAllforHARF(2, "QLSA", 11, 0.7);
		System.out.println("== SUT2: OARF related results ==");
		core.Metrics.showAllforOARF(2, "QLSA", 11, 0.7);
		System.out.println("[REPRODUCE] Final scores computed for the pricing model QLSA.");
		
		/*** Export of data files for reproducing the Figures up to Fig. 13 in the paper ***/
		System.out.println("\n====== Exporting data files for reproducing the Figures in the paper ======");
		example.DataForFigures.exportDataForAllFigure_to_12("files");
		
		// ****** Compare SUT2b and SUT3b
		
		// Load the generic performance SLOs generated in Step 2 and compute the performance SLOs for each tenant 
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT2.csv", "SUT2");
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT3.csv", "SUT3");
		tools.PerfSLOsPerTenant.ComputePerfSLOs("SUT2", "SUT3", "files\\benchmarkInputFiles\\PriorityTRT.csv");
		tools.PerfSLOsPerTenant.computePerfSLOs_per_tenant("files\\6tenants.csv");
		System.out.println("[REPRODUCE] Performance SLOs are computed for all tenants.");
		
		// Compute intermediate metrics for SUT2b and SUT3 using the pricing model QLSA
		core.PricingModelQLSA.computeIntermediateMetrics(2, 11, 18, 1, 20, false);
		core.PricingModelQLSA.computeIntermediateMetrics(3, 11, 18, 10, 20, false);
		System.out.println("[REPRODUCE] Intermediate metrics computed for the pricing model QLSA.");
		
		// Compute the final scores of SUT2b and SUT3 using the pricing model QLSA under the clusterSize 11 with SSR_MIN = 0.7
		System.out.println("[REPRODUCE] Computing final scores for the pricing model QLSA...");
		System.out.println("== SUT2b: HARF related resqults ==");
		core.Metrics.showAllforHARF(2, "QLSA", 11, 0.7);
		System.out.println("== SUT2b: OARF related results ==");
		core.Metrics.showAllforOARF(2, "QLSA", 11, 0.7);
		System.out.println("== SUT3: HARF related results ==");
		core.Metrics.showAllforHARF(3, "QLSA", 11, 0.7);
		System.out.println("== SUT3: OARF related results ==");
		core.Metrics.showAllforOARF(3, "QLSA", 11, 0.7);
		System.out.println("[REPRODUCE] Final scores computed for the pricing model QLSA.");
		
		/*** Export of data files for reproducing the Figures in the paper ***/
		System.out.println("\n====== Exporting data files for reproducing the Figures in the paper ======");
		example.DataForFigures.exportDataForFigure13ref("files");
		
		
		// Compute intermediate metrics for SUT2b+ and SUT3 using the pricing model QLSA
		core.PricingModelQLSA.computeIntermediateMetrics(2, 11, 18, 1, 60, false);
		core.PricingModelQLSA.computeIntermediateMetrics(3, 11, 18, 10, 20, false);
		System.out.println("[REPRODUCE] Intermediate metrics computed for the pricing model QLSA.");
		
		// Compute the final scores of SUT2b+ and SUT3 using the pricing model QLSA under the clusterSize 11 with SSR_MIN = 0.7
		System.out.println("[REPRODUCE] Computing final scores for the pricing model QLSA...");
		System.out.println("== SUT2b+: HARF related resqults ==");
		core.Metrics.showAllforHARF(2, "QLSA", 11, 0.7);
		System.out.println("== SUT2b+: OARF related results ==");
		core.Metrics.showAllforOARF(2, "QLSA", 11, 0.7);
		System.out.println("== SUT3: HARF related results ==");
		core.Metrics.showAllforHARF(3, "QLSA", 11, 0.7);
		System.out.println("== SUT3: OARF related results ==");
		core.Metrics.showAllforOARF(3, "QLSA", 11, 0.7);
		System.out.println("[REPRODUCE] Final scores computed for the pricing model QLSA.");
		
		/*** Export of data files for reproducing the Figures in the paper ***/
		System.out.println("\n====== Exporting data files for reproducing the Figures in the paper ======");
		example.DataForFigures.exportDataForFigure13("files");
		
		// ****** compare SUT3a and SUT4a by CAB
		// Merge the individual CAB traces
		CABTracesProcessing.MergeCABTraces(3, "files\\CABTraces\\SUT3", 20, "files\\CABTraces\\SUT3");
		System.out.println("[REPRODUCE] CAB traces for SUT3 merged.");
		CABTracesProcessing.MergeCABTraces(4, "files\\CABTraces\\SUT4", 20, "files\\CABTraces\\SUT4");
		System.out.println("[REPRODUCE] CAB traces for SUT4 merged.");

		/*** Export of data files for reproducing the Figures in the paper ***/
		System.out.println("\n====== Exporting data files for reproducing the Figures in the paper ======");
		example.DataForFigures.exportDataForFigure14("files");
		
		// ****** compare SUT3a and SUT4a by MTD-DS
		// Load the generic performance SLOs generated in Step 2 and compute the performance SLOs for each tenant 
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT3.csv", "SUT3");
		tools.PerfSLOsPerTenant.LoadPerfSLOs("files\\step2PerfSLOs\\perfSLOs_SUT4.csv", "SUT4");
		tools.PerfSLOsPerTenant.ComputePerfSLOs("SUT3", "SUT4", "files\\benchmarkInputFiles\\PriorityTRT.csv");
		tools.PerfSLOsPerTenant.computePerfSLOs_per_tenant("files\\6tenants.csv");
		System.out.println("[REPRODUCE] Performance SLOs are computed for all tenants.");
		
		// Compute intermediate metrics for SUT3a and SUT4a using the pricing model RCB
		core.PricingModelRCB.computeIntermediateMetrics(3, 11, 20, 10, 20, false);
		core.PricingModelRCB.computeIntermediateMetrics(4, 11, 20, 10, 20, true);
		System.out.println("[REPRODUCE] Intermediate metrics computed for the pricing model RCB.");
		
		// Compute the final scores of SUT3a and SUT4a using the pricing model RCB under the clusterSize 11 with SSR_MIN = 0.7
		System.out.println("[REPRODUCE] Computing final scores for the pricing model RCB...");
		System.out.println("== SUT3a: HARF related resqults ==");
		core.Metrics.showAllforHARF(3, "RCB", 11, 0.7);
		System.out.println("== SUT3a: OARF related results ==");
		core.Metrics.showAllforOARF(3, "RCB", 11, 0.7);
		System.out.println("== SUT4a: HARF related results ==");
		core.Metrics.showAllforHARF(4, "RCB", 11, 0.7);
		System.out.println("== SUT4a: OARF related results ==");
		core.Metrics.showAllforOARF(4, "RCB", 11, 0.7);
		System.out.println("[REPRODUCE] Final scores computed for the pricing model RCB.");
		
		/*** Export of data files for reproducing the Figures in the paper ***/
		System.out.println("\n====== Exporting data files for reproducing the Figures in the paper ======");
		example.DataForFigures.exportDataForFigure15("files");

		System.out.println("[REPRODUCE] Finished.");
	}
}
